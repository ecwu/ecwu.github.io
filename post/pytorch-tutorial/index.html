<!doctype html><html lang=en><head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1,trust:e=>["\\htmlId","\\href"].includes(e.command),macros:{"\\eqref":"\\href{###1}{(\\text{#1})}","\\ref":"\\href{###1}{\\text{#1}}","\\label":"\\htmlId{#1}{}"}})})</script><link rel=icon type=image/x-icon href=https://ecwuuuuu.com/favicon.ico><link rel=stylesheet href=https://ecwuuuuu.com/css/style.min.2142b911d0700303dca611947939e24eb6e948aa4052cb38c301dfdb1ba00d2c.css integrity="sha256-IUK5EdBwAwPcphGUeTniTrbpSKpAUss4wwHf2xugDSw="><meta charset=utf-8><title>PyTorch Tutorial - ECWU's Notebook</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Homepage and blog for Zhenghao Wu (ECWU), sharing thoughts on technology, everyday experiences, and photography. Engaging with the community through knowledge sharing."><meta name=keywords content="PyTorch,Machine Learning,Tensorflow,Model,DataLoader,Dataset,MLP,Transfer Learning,Tutorial,Deep Learning"><meta name=author content="ECWUUUUU"><script>(function(){const e=localStorage.getItem("current-theme");e===null?window.matchMedia("(prefers-color-scheme: dark)").matches?(localStorage.setItem("current-theme","dark"),document.documentElement.classList.add("dark")):(localStorage.setItem("current-theme","light"),document.documentElement.classList.remove("dark")):e==="dark"?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")})(),window.updateMermaidTheme=()=>{if(typeof mermaid!="undefined"){const e=document.documentElement.classList.contains("dark"),t=document.querySelectorAll("pre.mermaid");t.forEach(e=>{e.getAttribute("data-processed")?(e.removeAttribute("data-processed"),e.innerHTML=e.getAttribute("data-graph")):e.setAttribute("data-graph",e.textContent)}),e?(initMermaidDark(),mermaid.run()):(initMermaidLight(),mermaid.run())}};function change_color_mode(){localStorage.getItem("current-theme")==="light"?(document.documentElement.classList.add("dark"),localStorage.setItem("current-theme","dark"),change_discussion_color_mode(),updateMermaidTheme()):(document.documentElement.classList.remove("dark"),localStorage.setItem("current-theme","light"),change_discussion_color_mode(),updateMermaidTheme())}function change_discussion_color_mode(){const t=localStorage.getItem("current-theme")==="light"?"light":"dark",n={setConfig:{theme:t}},e=document.querySelector("iframe.giscus-frame");e&&e.contentWindow.postMessage({giscus:n},"https://giscus.app")}document.addEventListener("DOMContentLoaded",()=>{document.getElementById("header-theme-button").addEventListener("click",change_color_mode)})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1,trust:e=>["\\htmlId","\\href"].includes(e.command),macros:{"\\eqref":"\\href{###1}{(\\text{#1})}","\\ref":"\\href{###1}{\\text{#1}}","\\label":"\\htmlId{#1}{}"}})})</script><link rel=icon type=image/x-icon href=https://ecwuuuuu.com/favicon.ico><link rel=stylesheet href=https://ecwuuuuu.com/css/style.min.2142b911d0700303dca611947939e24eb6e948aa4052cb38c301dfdb1ba00d2c.css integrity="sha256-IUK5EdBwAwPcphGUeTniTrbpSKpAUss4wwHf2xugDSw="><meta charset=utf-8><title>PyTorch Tutorial - ECWU's Notebook</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Homepage and blog for Zhenghao Wu (ECWU), sharing thoughts on technology, everyday experiences, and photography. Engaging with the community through knowledge sharing."><meta name=keywords content="PyTorch,Machine Learning,Tensorflow,Model,DataLoader,Dataset,MLP,Transfer Learning,Tutorial,Deep Learning"><meta name=author content="ECWUUUUU"><script>(function(){const e=localStorage.getItem("current-theme");e===null?window.matchMedia("(prefers-color-scheme: dark)").matches?(localStorage.setItem("current-theme","dark"),document.documentElement.classList.add("dark")):(localStorage.setItem("current-theme","light"),document.documentElement.classList.remove("dark")):e==="dark"?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")})(),window.updateMermaidTheme=()=>{if(typeof mermaid!="undefined"){const e=document.documentElement.classList.contains("dark"),t=document.querySelectorAll("pre.mermaid");t.forEach(e=>{e.getAttribute("data-processed")?(e.removeAttribute("data-processed"),e.innerHTML=e.getAttribute("data-graph")):e.setAttribute("data-graph",e.textContent)}),e?(initMermaidDark(),mermaid.run()):(initMermaidLight(),mermaid.run())}};function change_color_mode(){localStorage.getItem("current-theme")==="light"?(document.documentElement.classList.add("dark"),localStorage.setItem("current-theme","dark"),change_discussion_color_mode(),updateMermaidTheme()):(document.documentElement.classList.remove("dark"),localStorage.setItem("current-theme","light"),change_discussion_color_mode(),updateMermaidTheme())}function change_discussion_color_mode(){const t=localStorage.getItem("current-theme")==="light"?"light":"dark",n={setConfig:{theme:t}},e=document.querySelector("iframe.giscus-frame");e&&e.contentWindow.postMessage({giscus:n},"https://giscus.app")}document.addEventListener("DOMContentLoaded",()=>{document.getElementById("header-theme-button").addEventListener("click",change_color_mode)})</script></head><body class="transition-colors duration-300"><div class=pt-6><div class="nav-title flex flex-row flex-wrap gap-2"><a href=https://ecwuuuuu.com/><svg class="w-36 fill-gray-900 dark:fill-gray-200" fill="currentColor" viewBox="0 0 204.24 43.44"><path d="M0 43.08V.36h18.24v5.76H6.12v12.54h10.56v5.76H6.12v12.54h12.12v6.12H0z"/><path d="M41.64 30.9v2.64c0 1.32-.25 2.57-.75 3.75s-1.19 2.23-2.07 3.15-1.91 1.65-3.09 2.19-2.45.81-3.81.81c-1.16.0-2.34-.16-3.54-.48s-2.28-.88-3.24-1.68-1.75-1.83-2.37-3.09c-.62-1.26-.93-2.87-.93-4.83V9.84c0-1.4.24-2.7.72-3.9s1.16-2.24 2.04-3.12 1.93-1.57 3.15-2.07C28.97.25 30.32.0 31.8.0c2.88.0 5.22.94 7.02 2.82.88.92 1.57 2.01 2.07 3.27s.75 2.63.75 4.11v2.4h-6.12v-2.04c0-1.2-.34-2.24-1.02-3.12s-1.6-1.32-2.76-1.32c-1.52.0-2.53.47-3.03 1.41s-.75 2.13-.75 3.57v21.84c0 1.24.27 2.28.81 3.12s1.51 1.26 2.91 1.26c.4.0.83-.07 1.29-.21s.89-.37 1.29-.69c.36-.32.66-.76.9-1.32s.36-1.26.36-2.1v-2.1h6.12z"/><path d="M78.36.36 72 43.08h-5.76l-4.68-27.72h-.12l-4.62 27.72h-5.76L44.7.36h6.48l3.06 27.12h.12l4.8-27.12h4.68l4.98 27.78h.12l2.94-27.78H78.36z"/><path d="M102 .36v33.12c0 1.4-.25 2.69-.75 3.87s-1.21 2.23-2.13 3.15-1.98 1.64-3.18 2.16-2.48.78-3.84.78-2.63-.26-3.81-.78-2.23-1.24-3.15-2.16c-.92-.92-1.64-1.97-2.16-3.15s-.78-2.47-.78-3.87V.36h6.12v32.52c0 1.52.36 2.64 1.08 3.36s1.62 1.08 2.7 1.08 1.98-.36 2.7-1.08 1.08-1.84 1.08-3.36V.36H102z"/><path d="M127.56.36v33.12c0 1.4-.25 2.69-.75 3.87s-1.21 2.23-2.13 3.15-1.98 1.64-3.18 2.16-2.48.78-3.84.78-2.63-.26-3.81-.78-2.23-1.24-3.15-2.16c-.92-.92-1.64-1.97-2.16-3.15s-.78-2.47-.78-3.87V.36h6.12v32.52c0 1.52.36 2.64 1.08 3.36s1.62 1.08 2.7 1.08 1.98-.36 2.7-1.08 1.08-1.84 1.08-3.36V.36h6.12z"/><path d="M153.12.36v33.12c0 1.4-.25 2.69-.75 3.87s-1.21 2.23-2.13 3.15-1.98 1.64-3.18 2.16c-1.2.52-2.48.78-3.84.78s-2.63-.26-3.81-.78-2.23-1.24-3.15-2.16c-.92-.92-1.64-1.97-2.16-3.15s-.78-2.47-.78-3.87V.36h6.12v32.52c0 1.52.36 2.64 1.08 3.36s1.62 1.08 2.7 1.08 1.98-.36 2.7-1.08S147 34.4 147 32.88V.36h6.12z"/><path d="M178.68.36v33.12c0 1.4-.25 2.69-.75 3.87s-1.21 2.23-2.13 3.15-1.98 1.64-3.18 2.16c-1.2.52-2.48.78-3.84.78s-2.63-.26-3.81-.78-2.23-1.24-3.15-2.16c-.92-.92-1.64-1.97-2.16-3.15s-.78-2.47-.78-3.87V.36H165v32.52c0 1.52.36 2.64 1.08 3.36s1.62 1.08 2.7 1.08c1.08.0 1.98-.36 2.7-1.08s1.08-1.84 1.08-3.36V.36h6.12z"/><path d="M204.24.36v33.12c0 1.4-.25 2.69-.75 3.87s-1.21 2.23-2.13 3.15-1.98 1.64-3.18 2.16c-1.2.52-2.48.78-3.84.78s-2.63-.26-3.81-.78-2.23-1.24-3.15-2.16c-.92-.92-1.64-1.97-2.16-3.15s-.78-2.47-.78-3.87V.36h6.12v32.52c0 1.52.36 2.64 1.08 3.36s1.62 1.08 2.7 1.08 1.98-.36 2.7-1.08 1.08-1.84 1.08-3.36V.36h6.12z"/></svg><p class=sr-only>ECWU Homepage</p></a></div><nav aria-label=navbar class="mb-3 mr-8"><ol role=list class="flex flex-wrap"><li><div class="flex items-center justify-center"><a href=https://ecwuuuuu.com/ class='mr-2 font-medium text-gray-900 dark:text-gray-200 visited:text-gray-900 dark:visited:text-gray-200'>Home</a>
<svg width="16" height="20" viewBox="0 0 16 20" fill="currentColor" aria-hidden="true" class="mr-2 w-4 h-5 fill-gray-300"><path d="M5.697 4.34 8.98 16.532h1.327L7.025 4.341H5.697z"/></svg></div></li><li><div class="flex items-center justify-center"><a href=https://ecwuuuuu.com/post/ class='mr-2 font-medium text-gray-900 dark:text-gray-200 visited:text-gray-900 dark:visited:text-gray-200'>Posts</a>
<svg width="16" height="20" viewBox="0 0 16 20" fill="currentColor" aria-hidden="true" class="mr-2 w-4 h-5 fill-gray-300"><path d="M5.697 4.34 8.98 16.532h1.327L7.025 4.341H5.697z"/></svg></div></li><li><div class="flex items-center justify-center"><a href=https://ecwuuuuu.com/photography/ class='mr-2 font-medium text-gray-900 dark:text-gray-200 visited:text-gray-900 dark:visited:text-gray-200'>Photos</a>
<svg width="16" height="20" viewBox="0 0 16 20" fill="currentColor" aria-hidden="true" class="mr-2 w-4 h-5 fill-gray-300"><path d="M5.697 4.34 8.98 16.532h1.327L7.025 4.341H5.697z"/></svg></div></li><li><div class="flex items-center justify-center"><a href=https://ecwuuuuu.com/series/ class='mr-2 font-medium text-gray-900 dark:text-gray-200 visited:text-gray-900 dark:visited:text-gray-200'>Series</a>
<svg width="16" height="20" viewBox="0 0 16 20" fill="currentColor" aria-hidden="true" class="mr-2 w-4 h-5 fill-gray-300"><path d="M5.697 4.34 8.98 16.532h1.327L7.025 4.341H5.697z"/></svg></div></li><li><div class="flex items-center justify-center"><a href=https://ecwuuuuu.com/page/about/ class='mr-2 font-medium text-gray-900 dark:text-gray-200 visited:text-gray-900 dark:visited:text-gray-200'>About</a>
<svg width="16" height="20" viewBox="0 0 16 20" fill="currentColor" aria-hidden="true" class="mr-2 w-4 h-5 fill-gray-300"><path d="M5.697 4.34 8.98 16.532h1.327L7.025 4.341H5.697z"/></svg></div></li><li><div class="flex items-center justify-center"><a href=https://ecwuuuuu.com/page/friends/ class='mr-2 font-medium text-gray-900 dark:text-gray-200 visited:text-gray-900 dark:visited:text-gray-200'>Friends</a></div></li></ol></nav><div class="flex items-center space-x-2"><div class="w-6 h-6 inline-flex items-center justify-center rounded-lg bg-white dark:bg-black shadow-inner ring-1 ring-black/5"><i id=header-theme-button data-feather=sun class="site-header-navlink rounded cursor-pointer"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-sun-icon lucide-sun h-4 w-4 block dark:hidden fill-gray-900 dark:fill-gray-200"><circle cx="12" cy="12" r="4"/><path d="M12 2v2"/><path d="M12 20v2"/><path d="m4.93 4.93 1.41 1.41"/><path d="m17.66 17.66 1.41 1.41"/><path d="M2 12h2"/><path d="M20 12h2"/><path d="m6.34 17.66-1.41 1.41"/><path d="m19.07 4.93-1.41 1.41"/></svg>
<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon-icon lucide-moon h-4 w-4 hidden dark:block fill-gray-900 dark:fill-gray-200"><path d="M20.985 12.486a9 9 0 11-9.473-9.472c.405-.022.617.46.402.803a6 6 0 008.268 8.268c.344-.215.825-.004.803.401"/></svg></i></div><span class="dark:hidden text-sm">To Dark Mode</span>
<span class="hidden dark:block dark:text-gray-200 text-sm">To Light Mode</span></div></div><figure class="ml-[-12.5vw] mt-12 mb-12 h-96 w-screen relative overflow-hidden"><img class="object-cover object-center w-full h-96" src="https://unsplash.com/photos/qHx3w6Gwz9k/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8MXx8UHlUb3JjaHxlbnwwfHx8fDE3MDEwNzEzNjh8MA&amp;force=true&amp;w=2400" alt="Featured Images"><figcaption class="text-sm absolute bottom-0 right-0 pr-5 bg-gray-900 text-gray-500 px-2">Photo by Brecht Corbeel on Unsplash</figcaption></figure><div class="mb-12 mt-16 w-content"><h1 class="text-gray-900 dark:text-gray-200">PyTorch Tutorial</h1><h2 class="text-gray-700 dark:text-gray-400">The material is prepared for people with basic machine learning foundations.</h2><div class="text-gray-700 dark:text-gray-400"><div class=space-y-1><p><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-signature-icon lucide-signature h-5 w-5 inline-block"><path d="m21 17-2.156-1.868A.5.5.0 0018 15.5v.5a1 1 0 01-1 1h-2a1 1 0 01-1-1c0-2.545-3.991-3.97-8.5-4a1 1 0 000 5c4.153.0 4.745-11.295 5.708-13.5a2.5 2.5.0 113.31 3.284"/><path d="M3 21h18"/></svg> Zhenghao Wu</p><p><span class=mr-2><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-calendar-clock-icon lucide-calendar-clock h-5 w-5 inline-block"><path d="M16 14v2.2l1.6 1"/><path d="M16 2v4"/><path d="M21 7.5V6a2 2 0 00-2-2H5A2 2 0 003 6v14a2 2 0 002 2h3.5"/><path d="M3 10h5"/><path d="M8 2v4"/><circle cx="16" cy="16" r="6"/></svg> Friday, January 5, 2024
</span><span><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clipboard-clock-icon lucide-clipboard-clock h-5 w-5 inline-block"><path d="M16 14v2.2l1.6 1"/><path d="M16 4h2a2 2 0 012 2v.832"/><path d="M8 4H6A2 2 0 004 6v14a2 2 0 002 2h2"/><circle cx="16" cy="16" r="6"/><rect x="8" y="2" width="8" height="4" rx="1"/></svg> 27 min read</span></p><p><span class=mr-2><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-flag-icon lucide-flag h-5 w-5 inline-block"><path d="M4 22V4a1 1 0 01.4-.8A6 6 0 018 2c3 0 5 2 7.333 2q2 0 3.067-.8A1 1 0 0120 4v10a1 1 0 01-.4.8A6 6 0 0116 16c-3 0-5-2-8-2a6 6 0 00-4 1.528"/></svg>
Status: Finished
</span><span><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-badge-check-icon lucide-badge-check h-5 w-5 inline-block"><path d="M3.85 8.62a4 4 0 014.78-4.77 4 4 0 016.74.0 4 4 0 014.78 4.78 4 4 0 010 6.74 4 4 0 01-4.77 4.78 4 4 0 01-6.75.0 4 4 0 01-4.78-4.77 4 4 0 010-6.76z"/><path d="m9 12 2 2 4-4"/></svg>
Confidence: 8</span></p><p><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-scroll-text-icon lucide-scroll-text h-5 w-5 inline-block"><path d="M15 12h-5"/><path d="M15 8h-5"/><path d="M19 17V5a2 2 0 00-2-2H4"/><path d="M8 21h12a2 2 0 002-2v-1a1 1 0 00-1-1H11a1 1 0 00-1 1v1a2 2 0 11-4 0V5A2 2 0 102 5v2a1 1 0 001 1h3"/></svg> <a href=#article-info-card>Post Details</a></p></div></div></div><main><article class="prose dark:prose-invert lg:prose-xl md:prose-lg prose-ul:leading-5 prose-ol:leading-5"><section class=mb-8><details closed class=toc><summary>Table of Contents</summary><nav id=TableOfContents><ul><li><a href=#introduction-to-pytorch>Introduction to PyTorch</a><ul><li><a href=#key-technology>Key Technology</a></li><li><a href=#comparison-with-other>Comparison with Other</a></li></ul></li><li><a href=#how-to-use-pytorch>How to use PyTorch</a><ul><li><a href=#online-computation-resource>Online Computation Resource</a></li><li><a href=#your-own-machine>Your Own Machine</a></li></ul></li><li><a href=#tensors-and-operations>Tensors and Operations</a><ul><li><a href=#tensor>Tensor</a></li><li><a href=#operations>Operations</a></li></ul></li><li><a href=#building-neural-networks>Building Neural Networks</a><ul><li><a href=#nnmodule-class><code>nn.Module</code> Class</a></li><li><a href=#predefined-layers>Predefined Layers</a></li><li><a href=#model-summary-and-parameters>Model Summary and Parameters</a></li></ul></li><li><a href=#dataset-and-dataloader>Dataset and DataLoader</a><ul><li><a href=#dataset-class><code>Dataset</code> Class</a></li><li><a href=#built-in-datasets>Built-in Datasets</a></li><li><a href=#transformations>Transformations</a></li><li><a href=#what-is-dataloader-and-how-to-use>What is DataLoader and how to use</a></li></ul></li><li><a href=#training-and-optimization>Training and Optimization</a><ul><li><a href=#defining-loss-functions-eg-cross-entropy-mean-squared-error>Defining loss functions (e.g., cross-entropy, mean squared error)</a></li><li><a href=#choosing-and-configuring-optimizers-eg-sgd-adam>Choosing and configuring optimizers (e.g., SGD, Adam)</a></li><li><a href=#do-training>Do Training</a></li></ul></li><li><a href=#transfer-learning>Transfer Learning</a><ul><li><a href=#load-models-and-pre-trained-weights>Load Models and Pre-trained weights</a></li><li><a href=#modify-model-for-your-task>Modify model for your task</a></li></ul></li><li><a href=#gpu-acceleration>GPU Acceleration</a></li><li><a href=#addition-topics>Addition Topics</a><ul><li><a href=#random-seed>Random Seed</a></li><li><a href=#dataloader-worker>DataLoader Worker</a></li></ul></li></ul></nav></details></section><blockquote><p>A little bit of context</p><p>This is the tutorial material I prepared in fall 2023, for people with basic foundation of machine learning to get hands-on PyTorch quickly.</p><p>My idea behind the whole document is to teach the reader how to do full model training from the ground up and introduce each component alongside the process.</p></blockquote><h2 id=introduction-to-pytorch>Introduction to PyTorch</h2><p><img src=https://pytorch.org/assets/images/logo-white.svg alt=PyTorch class=no-dark-invert></p><ul><li>PyTorch is an open-source deep learning framework that provides a flexible and dynamic approach to building and training neural networks.</li><li>Its popularity and widespread adoption by the research and industry communities.</li><li>PyTorch is widely known for its ease of use, Pythonic interface, and excellent support for research-oriented tasks.</li></ul><h3 id=key-technology>Key Technology</h3><h4 id=dynamic-computational-graph>Dynamic computational graph</h4><p>A <strong>computational graph</strong> represents the flow of data through a computational model in the form of a directed acyclic graph (DAG). It serves as a visual representation of the mathematical operations performed on input data to produce the desired output.</p><p><img src=http://cdn.ecwuuuuu.com/blog/image/pytorch-tutorial/computational_graph_equation2.jpg-compressed.webp alt=computational_graph_equation2.jpg></p><p>PyTorch allows for efficient and flexible model construction and dynamic control flow through its dynamic computational graph. Unlike TensorFlow, another popular deep-learning framework that employs static computational graphs, PyTorch constructs and executes the computational graph dynamically during runtime.</p><p>The dynamic nature of PyTorch&rsquo;s computational graph enables greater flexibility and control flow. The graph is built on-the-fly as operations are executed, making it easier to debug and write code that involves complex or varying control flows. This feature is particularly useful for tasks that require dynamic graph construction, like recurrent neural networks or models with varying input sizes. Additionally, PyTorch&rsquo;s dynamic nature facilitates seamless integration with Python control flow and external libraries. However, it&rsquo;s worth noting that the dynamic construction of the graph may result in reduced performance compared to static graphs.</p><p>In contrast, TensorFlow follows a static computational graph approach where the graph is defined and compiled before execution. The graph is constructed independently of the actual data being processed, which allows for potential optimization opportunities. Once the graph is defined, it can be executed repeatedly without the need for graph construction, thereby improving performance.</p><p>TensorFlow&rsquo;s static nature facilitates better graph optimization, including automatic differentiation and graph pruning. It is well-suited for scenarios where the model architecture is fixed and known in advance, with a focus on optimizing performance. However, the static nature of TensorFlow&rsquo;s graph construction may limit flexibility for models with dynamic control flow or varying input sizes. Writing code with TensorFlow&rsquo;s static graphs can sometimes be more complex and require additional boilerplate code.</p><blockquote><p>It&rsquo;s important to mention that both frameworks have evolved over time, with advancements made to enhance flexibility and efficiency. For instance, TensorFlow 2.0 introduced TensorFlow eager execution mode, which provides a dynamic execution similar to PyTorch. PyTorch also introduced TorchScript (<code>torch.JIT</code>) in PyTorch 2.0 for static graph optimization.</p></blockquote><h4 id=automatic-differentiation>Automatic differentiation</h4><p><a href=https://pytorch.org/blog/overview-of-pytorch-autograd-engine/>Overview of PyTorch Autograd Engine | PyTorch</a></p><p>PyTorch&rsquo;s automatic differentiation is a fundamental feature that enables efficient computation of gradients for training deep learning models. It is tightly integrated with PyTorch&rsquo;s <em>dynamic computational graph</em>, allowing for easy and efficient backpropagation of gradients through the network.</p><p>During the forward pass, PyTorch tracks <strong>operations on tensors</strong>, building a dynamic computational graph as these operations are executed. This graph <strong>records the operations and their dependencies</strong>, forming a directed acyclic graph (DAG). Each node in the graph represents an operation, and the edges represent data dependencies.</p><p>PyTorch also keeps track of the operations required <strong>for computing gradients</strong> during the forward pass. It creates a backward pass function for each operation, which calculates the gradient of the output with respect to the input tensors using the chain rule of calculus. During the backward pass, these backward pass functions are invoked to compute gradients efficiently.</p><p>The dynamic nature of PyTorch&rsquo;s computational graph allows for the automatic differentiation process to be performed on a per-graph basis. Gradients are computed dynamically as the forward pass is executed. This dynamic approach offers greater flexibility and control over the model&rsquo;s behavior, as the graph can change at runtime based on the data or control flow.</p><p>To perform backpropagation in PyTorch, you typically define a loss function and call the <code>backward()</code> method on the loss tensor. <strong>This triggers the computation of gradients using the dynamic computational graph and the chain rule, updating the gradients of all the model&rsquo;s parameters</strong>. These gradients can then be used to update the model&rsquo;s weights using an optimizer, such as stochastic gradient descent (SGD).</p><p>Overall, PyTorch&rsquo;s automatic differentiation capability, combined with its dynamic computational graph, simplifies the process of computing gradients and enables efficient backpropagation for training deep learning models.</p><h3 id=comparison-with-other>Comparison with Other</h3><p>These figures show the usage of DL frameworks based on PyPI downloads, the number of models available on HuggingFace, and the changes in publications over time.</p><p><img src=http://cdn.ecwuuuuu.com/blog/image/pytorch-tutorial/num_hf_models_2023.png-compressed.webp alt=num_hf_models_2023.png></p><p><img src=http://cdn.ecwuuuuu.com/blog/image/pytorch-tutorial/num_top_models_2023.png-compressed.webp alt=num_top_models_2023.png></p><p><img src=http://cdn.ecwuuuuu.com/blog/image/pytorch-tutorial/Fraction-of-Papers-Using-PyTorch-vs.-TensorFlow.png-compressed.webp alt=Fraction-of-Papers-Using-PyTorch-vs.-TensorFlow.png></p><p><img src=http://cdn.ecwuuuuu.com/blog/image/pytorch-tutorial/torch-download-trend.png-compressed.webp alt=torch-download-trend.png></p><p><img src=http://cdn.ecwuuuuu.com/blog/image/pytorch-tutorial/tensorflow-download-trend.png-compressed.webp alt=tensorflow-download-trend.png></p><h2 id=how-to-use-pytorch>How to use PyTorch</h2><p>To use PyTorch, you have different options depending on the environment you are working in.</p><h3 id=online-computation-resource>Online Computation Resource</h3><p>If you are using platforms like Kaggle, Google Colab, or any other similar environment, you typically don&rsquo;t need to set up the PyTorch environment explicitly. These platforms provide a containerized environment with the necessary computational resources and basic environment already installed.</p><ul><li><a href=https://www.kaggle.com/>Kaggle: Your Machine Learning and Data Science Community</a></li><li><a href=https://colab.research.google.com/>colab.google</a></li></ul><h3 id=your-own-machine>Your Own Machine</h3><p>If you want to work on your own machine, you can follow these steps:</p><ul><li>Ensure you have Python installed on your machine. You can use Anaconda or Miniconda Python distributions: <a href=https://docs.conda.io/projects/miniconda/en/latest/>Anaconda/Miniconda Miniconda — miniconda documentation</a></li><li>To install PyTorch, you can either use pip or conda: <a href=https://pytorch.org/get-started/locally/>Start Locally | PyTorch</a></li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-bash data-lang=bash><span style=display:flex><span>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
</span></span><span style=display:flex><span><span style=color:#586e75># or</span>
</span></span><span style=display:flex><span>conda install pytorch torchvision torchaudio cpuonly -c pytorch
</span></span></code></pre></div><p>This command will download and install the CPU version of PyTorch.</p><h4 id=with-gpu>With GPU</h4><p>If you have a GPU and want to utilize its power for accelerated computations, make sure you have a GPU that supports CUDA 11 or later.</p><ul><li>CUDA Environment: <a href=https://developer.nvidia.com/cuda-zone>CUDA Zone - Library of Resources | NVIDIA Developer</a></li><li>To install the GPU version of PyTorch, use pip with the following command:</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-bash data-lang=bash><span style=display:flex><span>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
</span></span></code></pre></div><p>These steps will help you set up PyTorch based on your specific requirements and environment.</p><hr><h2 id=tensors-and-operations>Tensors and Operations</h2><h3 id=tensor>Tensor</h3><p>Tensors are a specialized data structure that are very similar to arrays and matrices (multi-dimensional array). In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.</p><p>Tensor can have different data types such as float, integer, or boolean, which is similar to NumPy’s <code>ndarray</code> and, except that <strong>tensors can run on GPUs or other hardware accelerators</strong>. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data. Tensors are also <strong>optimized for automatic differentiation</strong>.</p><h4 id=initializing-a-tensor>Initializing a Tensor</h4><ul><li>Direct: You can initialize a tensor of a specific size with all elements set to zero using the torch.zeros() function.</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#586e75># Initialize a tensor of size 2x3 with all elements set to zero</span>
</span></span><span style=display:flex><span>tensor_a <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>zeros(<span style=color:#2aa198>2</span>, <span style=color:#2aa198>3</span>)
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(tensor_a)
</span></span></code></pre></div><ul><li>From NumPy: You can create a NumPy array and then initialize a tensor from it using the <code>torch.from_numpy()</code> function.</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>import</span> numpy <span style=color:#719e07>as</span> np
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Create a NumPy array</span>
</span></span><span style=display:flex><span>numpy_array <span style=color:#719e07>=</span> np<span style=color:#719e07>.</span>array([[<span style=color:#2aa198>1</span>, <span style=color:#2aa198>2</span>, <span style=color:#2aa198>3</span>], [<span style=color:#2aa198>4</span>, <span style=color:#2aa198>5</span>, <span style=color:#2aa198>6</span>]])
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Initialize a tensor from the NumPy array</span>
</span></span><span style=display:flex><span>tensor_b <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>from_numpy(numpy_array)
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(tensor_b)
</span></span></code></pre></div><ul><li>From another tensor: You can create a new tensor from an existing tensor using the <code>torch.tensor()</code> function.</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#586e75># From another tensor</span>
</span></span><span style=display:flex><span>tensor_c <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>tensor([[<span style=color:#2aa198>1</span>, <span style=color:#2aa198>2</span>, <span style=color:#2aa198>3</span>], [<span style=color:#2aa198>4</span>, <span style=color:#2aa198>5</span>, <span style=color:#2aa198>6</span>]])
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># When you use torch.tensor(), you create a new tensor</span>
</span></span><span style=display:flex><span><span style=color:#586e75># without any connection to the original tensor&#39;s computational</span>
</span></span><span style=display:flex><span><span style=color:#586e75># graph or gradient history. This means that the new tensor will</span>
</span></span><span style=display:flex><span><span style=color:#586e75># not have any gradients associated with it, and it will not</span>
</span></span><span style=display:flex><span><span style=color:#586e75># participate in any future gradient calculations or</span>
</span></span><span style=display:flex><span><span style=color:#586e75># backpropagation.</span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># tensor_d = torch.tensor(tensor_c)</span>
</span></span><span style=display:flex><span><span style=color:#586e75># print(tensor_d)</span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>tensor_e <span style=color:#719e07>=</span> tensor_c<span style=color:#719e07>.</span>clone()<span style=color:#719e07>.</span>detach()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(tensor_e)
</span></span></code></pre></div><ul><li>Fill in random or constant values: You can initialize a tensor with random values. You can also initialize a tensor with constant values using <code>torch.full()</code>.</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#586e75># Fill in random or constant values</span>
</span></span><span style=display:flex><span>tensor_f <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>rand(<span style=color:#2aa198>3</span>, <span style=color:#2aa198>2</span>)  <span style=color:#586e75># Uniform Distribution</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(tensor_f)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>tensor_g <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>randn(<span style=color:#2aa198>2</span>, <span style=color:#2aa198>2</span>)  <span style=color:#586e75># Normal Distribution</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(tensor_g)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>tensor_h <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>full((<span style=color:#2aa198>3</span>, <span style=color:#2aa198>3</span>), <span style=color:#2aa198>7</span>)  <span style=color:#586e75># Filled with constant value</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(tensor_h)
</span></span></code></pre></div><h4 id=attributes-of-a-tensor>Attributes of a Tensor</h4><p>Tensors in PyTorch have attributes that provide information about their shape, data type, device, and gradient requirements. Some commonly used attributes include:</p><ul><li>Shape: It represents the size of each dimension of the tensor. You can access it using the shape attribute or the <code>size()</code> method. For example, <code>tensor.shape</code> or <code>tensor.size()</code> will return the shape of the tensor.</li><li>Datatype: It indicates the data type of the elements stored in the tensor. PyTorch supports various data types such as <code>torch.float32</code>, <code>torch.int64</code>, <code>torch.bool</code>, etc. You can access the data type of a tensor using the <code>dtype</code> attribute.</li><li>Device: It specifies the device (CPU or GPU) on which the tensor is stored. You can use the device attribute to check if the tensor is on the CPU or GPU.</li><li><code>requires_grad</code>: This attribute indicates whether the tensor requires gradient computation for automatic differentiation. By default, tensors created directly from data do not require gradients. You can enable gradient tracking by setting <code>requires_grad=True</code> on a tensor.</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>tensor <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>zeros(<span style=color:#2aa198>2</span>, <span style=color:#2aa198>3</span>, <span style=color:#2aa198>4</span>)
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(<span style=color:#2aa198>f</span><span style=color:#2aa198>&#34;tensor shape: </span><span style=color:#2aa198>{</span>tensor<span style=color:#719e07>.</span>shape<span style=color:#2aa198>}</span><span style=color:#2aa198>&#34;</span>)  <span style=color:#586e75># This is a attribute, not function</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(<span style=color:#2aa198>f</span><span style=color:#2aa198>&#34;tensor size: </span><span style=color:#2aa198>{</span>tensor<span style=color:#719e07>.</span>size()<span style=color:#2aa198>}</span><span style=color:#2aa198>&#34;</span>)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(<span style=color:#2aa198>f</span><span style=color:#2aa198>&#34;tensor dtype: </span><span style=color:#2aa198>{</span>tensor<span style=color:#719e07>.</span>dtype<span style=color:#2aa198>}</span><span style=color:#2aa198>&#34;</span>)
</span></span><span style=display:flex><span>tensor <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>zeros(<span style=color:#2aa198>2</span>, <span style=color:#2aa198>3</span>, dtype<span style=color:#719e07>=</span>torch<span style=color:#719e07>.</span>float64)  <span style=color:#586e75># explicite define the dtype for tensor</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(<span style=color:#2aa198>f</span><span style=color:#2aa198>&#34;tensor dtype: </span><span style=color:#2aa198>{</span>tensor<span style=color:#719e07>.</span>dtype<span style=color:#2aa198>}</span><span style=color:#2aa198>&#34;</span>)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># tensor = torch.zeros(2, 3).to(&#39;cuda&#39;)</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(<span style=color:#2aa198>f</span><span style=color:#2aa198>&#34;tensor device: </span><span style=color:#2aa198>{</span>tensor<span style=color:#719e07>.</span>device<span style=color:#2aa198>}</span><span style=color:#2aa198>&#34;</span>)
</span></span></code></pre></div><h3 id=operations>Operations</h3><p>Tensors in PyTorch support a wide range of operations for manipulating and performing computations on the data they contain. These operations include arithmetic operations, linear algebra, matrix manipulation (such as indexing and slicing), and more. You can refer to the <a href=https://pytorch.org/docs/stable/tensors.html>PyTorch documentation</a> for a comprehensive list of tensor operations.</p><p>Some examples of tensor operations include:</p><ul><li>Indexing and slicing: Tensors can be indexed and sliced using similar syntax as NumPy arrays.</li><li>Joining tensors: Tensors can be concatenated or stacked along different dimensions.</li><li>Reshaping: Tensors can be reshaped to have a different shape but the same data.</li><li>Arithmetic operations: Tensors support element-wise addition, multiplication, and matrix multiplication.</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>tensor <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>tensor([[<span style=color:#2aa198>1</span>, <span style=color:#2aa198>2</span>, <span style=color:#2aa198>3</span>], [<span style=color:#2aa198>4</span>, <span style=color:#2aa198>5</span>, <span style=color:#2aa198>6</span>]])
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Indexing</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(tensor[<span style=color:#2aa198>0</span>, <span style=color:#2aa198>1</span>])  <span style=color:#586e75># Output: 2</span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Slicing</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(tensor[:, <span style=color:#2aa198>1</span>:])  <span style=color:#586e75># Output: tensor([[2, 3], [5, 6]])</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># Joining tensors</span>
</span></span><span style=display:flex><span>tensor1 <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>tensor([[<span style=color:#2aa198>1</span>, <span style=color:#2aa198>2</span>], [<span style=color:#2aa198>3</span>, <span style=color:#2aa198>4</span>]])
</span></span><span style=display:flex><span>tensor2 <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>tensor([[<span style=color:#2aa198>5</span>, <span style=color:#2aa198>6</span>], [<span style=color:#2aa198>7</span>, <span style=color:#2aa198>8</span>]])
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Concatenation</span>
</span></span><span style=display:flex><span>concatenated <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>cat((tensor1, tensor2), dim<span style=color:#719e07>=</span><span style=color:#2aa198>0</span>)  <span style=color:#586e75># Concatenate along dimension 0</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(concatenated)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Stacking</span>
</span></span><span style=display:flex><span>stacked <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>stack((tensor1, tensor2), dim<span style=color:#719e07>=</span><span style=color:#2aa198>0</span>)  <span style=color:#586e75># Stack along new dimension 0</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(stacked)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># Reshape: Returns a tensor with the same data but a different shape.</span>
</span></span><span style=display:flex><span>tensor <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>tensor([[<span style=color:#2aa198>1</span>, <span style=color:#2aa198>2</span>], [<span style=color:#2aa198>3</span>, <span style=color:#2aa198>4</span>]])
</span></span><span style=display:flex><span>reshaped <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>reshape(tensor, (<span style=color:#2aa198>4</span>,))
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(reshaped)
</span></span></code></pre></div><h4 id=arithmetic-operations>Arithmetic operations</h4><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>tensor1 <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>tensor([[<span style=color:#2aa198>1</span>, <span style=color:#2aa198>2</span>], [<span style=color:#2aa198>3</span>, <span style=color:#2aa198>4</span>]])
</span></span><span style=display:flex><span>tensor2 <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>tensor([[<span style=color:#2aa198>5</span>, <span style=color:#2aa198>6</span>], [<span style=color:#2aa198>7</span>, <span style=color:#2aa198>8</span>]])
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Element-wise addition</span>
</span></span><span style=display:flex><span>result <span style=color:#719e07>=</span> tensor1 <span style=color:#719e07>+</span> tensor2
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(result)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Element-wise multiplication</span>
</span></span><span style=display:flex><span>result <span style=color:#719e07>=</span> tensor1 <span style=color:#719e07>*</span> tensor2
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(result)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Matrix Multiplication</span>
</span></span><span style=display:flex><span>result <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>matmul(tensor1, tensor2)
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(result)
</span></span></code></pre></div><h2 id=building-neural-networks>Building Neural Networks</h2><h3 id=nnmodule-class><code>nn.Module</code> Class</h3><p>The <code>nn.Module</code> class serves as a base class for all neural network modules in PyTorch. It is used to define the architecture and behavior of the network. This class provides a convenient way to organize the parameters of a model and define the forward pass computation. To create your own neural network using <code>nn.Module</code>, you need to define a subclass of <code>nn.Module</code> and override two key methods: <code>__init__</code> and <code>forward</code>. The <code>__init__</code> method is used to define the layers and modules of your network, while the <code>forward</code> method specifies the forward pass computation.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>import</span> torch
</span></span><span style=display:flex><span><span style=color:#719e07>import</span> torch.nn <span style=color:#719e07>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>class</span> <span style=color:#268bd2>MyNetwork</span>(nn<span style=color:#719e07>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#719e07>def</span> <span style=color:#268bd2>__init__</span>(<span style=color:#268bd2>self</span>):
</span></span><span style=display:flex><span>        <span style=color:#b58900>super</span>(MyNetwork, <span style=color:#268bd2>self</span>)<span style=color:#719e07>.</span><span style=color:#268bd2>__init__</span>()
</span></span><span style=display:flex><span>        <span style=color:#586e75># Define the layers and modules of your network</span>
</span></span><span style=display:flex><span>        <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>fc1 <span style=color:#719e07>=</span> nn<span style=color:#719e07>.</span>Linear(<span style=color:#2aa198>10</span>, <span style=color:#2aa198>5</span>)
</span></span><span style=display:flex><span>        <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>fc2 <span style=color:#719e07>=</span> nn<span style=color:#719e07>.</span>Linear(<span style=color:#2aa198>5</span>, <span style=color:#2aa198>2</span>)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>    <span style=color:#719e07>def</span> <span style=color:#268bd2>forward</span>(<span style=color:#268bd2>self</span>, x):
</span></span><span style=display:flex><span>        <span style=color:#586e75># Define the forward pass computation</span>
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>fc1(x)
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>fc2(x)
</span></span><span style=display:flex><span>        <span style=color:#719e07>return</span> x
</span></span></code></pre></div><p><img src=http://cdn.ecwuuuuu.com/blog/image/pytorch-tutorial/mlp-figure.png-compressed.webp alt=mlp-figure.png></p><p>In the <code>__init__</code> method, you can define any layers or modules you need for your network. In this example, we define two fully connected layers (<code>nn.Linear</code>) with specified input and output sizes.
In the forward method, you specify the sequence of operations that will be applied to the input x during the forward pass. In this example, we apply the first linear layer (<code>self.fc1</code>), followed by the second linear layer (<code>self.fc2</code>).</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>batch_size <span style=color:#719e07>=</span> <span style=color:#2aa198>1</span>
</span></span><span style=display:flex><span>in_features <span style=color:#719e07>=</span> <span style=color:#2aa198>10</span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>model <span style=color:#719e07>=</span> MyNetwork()
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>input_data <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>randn(batch_size, in_features)
</span></span><span style=display:flex><span>output <span style=color:#719e07>=</span> model(input_data)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(<span style=color:#2aa198>f</span><span style=color:#2aa198>&#34;input: </span><span style=color:#2aa198>{</span>input_data<span style=color:#2aa198>}</span><span style=color:#cb4b16>\n</span><span style=color:#2aa198>output: </span><span style=color:#2aa198>{</span>output<span style=color:#2aa198>}</span><span style=color:#2aa198>&#34;</span>)
</span></span></code></pre></div><p>In this example, we first instantize the network we just defined. And setup the input_data which is a tensor representing the input to your network. You can pass this input tensor to your network instance (model) to obtain the output tensor (output). The forward method of your network will be automatically called, executing the forward pass computation defined earlier.</p><h3 id=predefined-layers>Predefined Layers</h3><p>PyTorch&rsquo;s nn module provides a wide range of predefined layers that you can use to build your neural networks. Just like the <code>nn.Linear</code> and <code>nn.ReLU</code> we just use. Here are some commonly used layers:</p><ul><li><code>nn.Linear</code>: This layer implements a fully connected (linear) operation. It applies a linear transformation to the input data, where each input element is multiplied by a weight and summed with a bias term.</li><li><code>nn.Conv2d</code>: This layer performs 2D convolutional operations on input data, commonly used in image processing tasks. It applies a set of learnable filters (kernels) to the input tensor to extract local features.
nn.Dropout: This layer implements dropout regularization, which randomly sets input elements to zero during training. Dropout helps prevent overfitting by reducing the interdependencies between neurons.</li><li><code>nn.BatchNorm2d</code>: This layer performs batch normalization along the channels of a 2D input tensor. It normalizes the input by subtracting the mean and dividing by the standard deviation, which helps stabilize and accelerate the training process.</li><li><code>nn.ReLU</code>: This activation function applies the Rectified Linear Unit (ReLU) element-wise to the input tensor. ReLU sets negative values to zero and keeps positive values unchanged.</li><li><code>nn.Softmax</code>: This activation function applies the softmax operation to the input tensor, which normalizes the tensor into a probability distribution over the classes. It is commonly used for multi-class classification problems.</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>class</span> <span style=color:#268bd2>MyNetwork</span>(nn<span style=color:#719e07>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#719e07>def</span> <span style=color:#268bd2>__init__</span>(<span style=color:#268bd2>self</span>, in_channels, out_classes):
</span></span><span style=display:flex><span>        <span style=color:#b58900>super</span>(MyNetwork, <span style=color:#268bd2>self</span>)<span style=color:#719e07>.</span><span style=color:#268bd2>__init__</span>()
</span></span><span style=display:flex><span>        <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>conv <span style=color:#719e07>=</span> nn<span style=color:#719e07>.</span>Conv2d(in_channels, <span style=color:#2aa198>64</span>, kernel_size<span style=color:#719e07>=</span><span style=color:#2aa198>3</span>, padding<span style=color:#719e07>=</span><span style=color:#2aa198>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>batchnorm <span style=color:#719e07>=</span> nn<span style=color:#719e07>.</span>BatchNorm2d(<span style=color:#2aa198>64</span>)
</span></span><span style=display:flex><span>        <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>relu <span style=color:#719e07>=</span> nn<span style=color:#719e07>.</span>ReLU()
</span></span><span style=display:flex><span>        <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>dropout <span style=color:#719e07>=</span> nn<span style=color:#719e07>.</span>Dropout(p<span style=color:#719e07>=</span><span style=color:#2aa198>0.2</span>)
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span>        <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>fc1 <span style=color:#719e07>=</span> nn<span style=color:#719e07>.</span>Linear(<span style=color:#2aa198>64</span> <span style=color:#719e07>*</span> <span style=color:#2aa198>28</span> <span style=color:#719e07>*</span> <span style=color:#2aa198>28</span>, <span style=color:#2aa198>128</span>)
</span></span><span style=display:flex><span>        <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>fc2 <span style=color:#719e07>=</span> nn<span style=color:#719e07>.</span>Linear(<span style=color:#2aa198>128</span>, out_classes)
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span>        <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>softmax <span style=color:#719e07>=</span> nn<span style=color:#719e07>.</span>Softmax(dim<span style=color:#719e07>=</span><span style=color:#2aa198>1</span>)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>    <span style=color:#719e07>def</span> <span style=color:#268bd2>forward</span>(<span style=color:#268bd2>self</span>, x):
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>conv(x)
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>batchnorm(x)
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>relu(x)
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>dropout(x)
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> x<span style=color:#719e07>.</span>view(x<span style=color:#719e07>.</span>size(<span style=color:#2aa198>0</span>), <span style=color:#719e07>-</span><span style=color:#2aa198>1</span>)  <span style=color:#586e75># Flatten the tensor</span>
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>fc1(x)
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>relu(x)
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>dropout(x)
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>fc2(x)
</span></span><span style=display:flex><span>        x <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>softmax(x)
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span>        <span style=color:#719e07>return</span> x
</span></span></code></pre></div><h3 id=model-summary-and-parameters>Model Summary and Parameters</h3><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#719e07>=</span> MyNetwork(in_channels<span style=color:#719e07>=</span><span style=color:#2aa198>1</span>, out_classes<span style=color:#719e07>=</span><span style=color:#2aa198>10</span>)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Print the network structure</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(model)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Calculate the parameter count</span>
</span></span><span style=display:flex><span>total_params <span style=color:#719e07>=</span> <span style=color:#b58900>sum</span>(p<span style=color:#719e07>.</span>numel() <span style=color:#719e07>for</span> p <span style=color:#719e07>in</span> model<span style=color:#719e07>.</span>parameters() <span style=color:#719e07>if</span> p<span style=color:#719e07>.</span>requires_grad)
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(<span style=color:#2aa198>f</span><span style=color:#2aa198>&#34;Total parameters: </span><span style=color:#2aa198>{</span>total_params<span style=color:#2aa198>}</span><span style=color:#2aa198>&#34;</span>)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#2aa198>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#2aa198>Output:
</span></span></span><span style=display:flex><span><span style=color:#2aa198> 
</span></span></span><span style=display:flex><span><span style=color:#2aa198>MyNetwork(
</span></span></span><span style=display:flex><span><span style=color:#2aa198>  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
</span></span></span><span style=display:flex><span><span style=color:#2aa198>  (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
</span></span></span><span style=display:flex><span><span style=color:#2aa198>  (relu): ReLU()
</span></span></span><span style=display:flex><span><span style=color:#2aa198>  (dropout): Dropout(p=0.2, inplace=False)
</span></span></span><span style=display:flex><span><span style=color:#2aa198>  (fc1): Linear(in_features=50176, out_features=128, bias=True)
</span></span></span><span style=display:flex><span><span style=color:#2aa198>  (fc2): Linear(in_features=128, out_features=10, bias=True)
</span></span></span><span style=display:flex><span><span style=color:#2aa198>  (softmax): Softmax(dim=1)
</span></span></span><span style=display:flex><span><span style=color:#2aa198>)
</span></span></span><span style=display:flex><span><span style=color:#2aa198>Total parameters: 6425866
</span></span></span><span style=display:flex><span><span style=color:#2aa198>&#34;&#34;&#34;</span>
</span></span></code></pre></div><p>There is a convenient package called <code>torchsummary</code> that you can use to calculate the total number of parameters in a PyTorch model. Here&rsquo;s how you can use it:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>from</span> torchsummary <span style=color:#719e07>import</span> summary
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Instantiate the network</span>
</span></span><span style=display:flex><span>model <span style=color:#719e07>=</span> MyNetwork(in_channels<span style=color:#719e07>=</span><span style=color:#2aa198>1</span>, out_classes<span style=color:#719e07>=</span><span style=color:#2aa198>10</span>)
</span></span><span style=display:flex><span>model <span style=color:#719e07>=</span> model<span style=color:#719e07>.</span>to(<span style=color:#2aa198>&#34;cuda:0&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#586e75># Print the model summary</span>
</span></span><span style=display:flex><span>summary(model, (<span style=color:#2aa198>1</span>, <span style=color:#2aa198>28</span>, <span style=color:#2aa198>28</span>))  <span style=color:#586e75># Provide an example input size</span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#2aa198>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#2aa198>Output:
</span></span></span><span style=display:flex><span><span style=color:#2aa198> 
</span></span></span><span style=display:flex><span><span style=color:#2aa198>----------------------------------------------------------------
</span></span></span><span style=display:flex><span><span style=color:#2aa198>        Layer (type)               Output Shape         Param #
</span></span></span><span style=display:flex><span><span style=color:#2aa198>================================================================
</span></span></span><span style=display:flex><span><span style=color:#2aa198>            Conv2d-1           [-1, 64, 28, 28]           1,792
</span></span></span><span style=display:flex><span><span style=color:#2aa198>       BatchNorm2d-2           [-1, 64, 28, 28]             128
</span></span></span><span style=display:flex><span><span style=color:#2aa198>              ReLU-3           [-1, 64, 28, 28]               0
</span></span></span><span style=display:flex><span><span style=color:#2aa198>           Dropout-4           [-1, 64, 28, 28]               0
</span></span></span><span style=display:flex><span><span style=color:#2aa198>            Linear-5                  [-1, 128]       6,422,656
</span></span></span><span style=display:flex><span><span style=color:#2aa198>              ReLU-6                  [-1, 128]               0
</span></span></span><span style=display:flex><span><span style=color:#2aa198>           Dropout-7                  [-1, 128]               0
</span></span></span><span style=display:flex><span><span style=color:#2aa198>            Linear-8                   [-1, 10]           1,290
</span></span></span><span style=display:flex><span><span style=color:#2aa198>           Softmax-9                   [-1, 10]               0
</span></span></span><span style=display:flex><span><span style=color:#2aa198>================================================================
</span></span></span><span style=display:flex><span><span style=color:#2aa198>Total params: 6,425,866
</span></span></span><span style=display:flex><span><span style=color:#2aa198>Trainable params: 6,425,866
</span></span></span><span style=display:flex><span><span style=color:#2aa198>Non-trainable params: 0
</span></span></span><span style=display:flex><span><span style=color:#2aa198>----------------------------------------------------------------
</span></span></span><span style=display:flex><span><span style=color:#2aa198>Input size (MB): 0.01
</span></span></span><span style=display:flex><span><span style=color:#2aa198>Forward/backward pass size (MB): 1.53
</span></span></span><span style=display:flex><span><span style=color:#2aa198>Params size (MB): 24.51
</span></span></span><span style=display:flex><span><span style=color:#2aa198>Estimated Total Size (MB): 26.06
</span></span></span><span style=display:flex><span><span style=color:#2aa198>----------------------------------------------------------------
</span></span></span><span style=display:flex><span><span style=color:#2aa198>&#34;&#34;&#34;</span>
</span></span></code></pre></div><h2 id=dataset-and-dataloader>Dataset and DataLoader</h2><p>PyTorch Dataset represents a dataset of input samples and their corresponding labels, while PyTorch DataLoader is responsible for efficiently loading the data from the dataset during training or inference.</p><h3 id=dataset-class><code>Dataset</code> Class</h3><p>The PyTorch <code>torch.utils.data.Dataset</code> class is a base class that you can inherit from to create your custom dataset.</p><p>To create a dataset in PyTorch, you need to override the following methods of the <code>Dataset</code> class:</p><ul><li><code>__len__</code>: Returns the total number of data samples in the dataset.</li><li><code>__getitem__</code>: Retrieves a specific data sample from the dataset, given its index.</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>import</span> torch
</span></span><span style=display:flex><span><span style=color:#719e07>from</span> torch.utils.data <span style=color:#719e07>import</span> Dataset
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#719e07>class</span> <span style=color:#268bd2>CustomDataset</span>(Dataset):
</span></span><span style=display:flex><span>    <span style=color:#719e07>def</span> <span style=color:#268bd2>__init__</span>(<span style=color:#268bd2>self</span>, data):
</span></span><span style=display:flex><span>        <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>data <span style=color:#719e07>=</span> data
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>    <span style=color:#719e07>def</span> <span style=color:#268bd2>__len__</span>(<span style=color:#268bd2>self</span>):
</span></span><span style=display:flex><span>        <span style=color:#719e07>return</span> <span style=color:#b58900>len</span>(<span style=color:#268bd2>self</span><span style=color:#719e07>.</span>data)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>    <span style=color:#719e07>def</span> <span style=color:#268bd2>__getitem__</span>(<span style=color:#268bd2>self</span>, index):
</span></span><span style=display:flex><span>        sample <span style=color:#719e07>=</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>data[index]
</span></span><span style=display:flex><span>        <span style=color:#586e75># Implement any necessary preprocessing or data transformations here</span>
</span></span><span style=display:flex><span>        <span style=color:#586e75># Return the preprocessed sample</span>
</span></span><span style=display:flex><span>        <span style=color:#719e07>return</span> sample
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Create an instance of your custom dataset</span>
</span></span><span style=display:flex><span>data <span style=color:#719e07>=</span> [<span style=color:#2aa198>1</span>, <span style=color:#2aa198>2</span>, <span style=color:#2aa198>3</span>, <span style=color:#2aa198>4</span>, <span style=color:#2aa198>5</span>]
</span></span><span style=display:flex><span>dataset <span style=color:#719e07>=</span> CustomDataset(data)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Access individual data samples</span>
</span></span><span style=display:flex><span>sample <span style=color:#719e07>=</span> dataset[<span style=color:#2aa198>0</span>]
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(sample)  <span style=color:#586e75># Output: 1</span>
</span></span></code></pre></div><h3 id=built-in-datasets>Built-in Datasets</h3><p>PyTorch provides several built-in datasets through the <code>torchvision</code>.datasets module. These datasets are commonly used for computer vision tasks. Here are some of the popular built-in datasets available in PyTorch:</p><ul><li><a href=http://yann.lecun.com/exdb/mnist/>MNIST</a>: Handwritten digit dataset.</li><li><a href=https://www.cs.toronto.edu/~kriz/cifar.html>CIFAR10 and CIFAR100</a>: Small images dataset with 10 and 100 classes, respectively.</li><li><a href=https://www.image-net.org/>ImageNet</a>: Large-scale image dataset with 1000 classes.</li><li><a href=https://github.com/zalandoresearch/fashion-mnist>FashionMNIST</a>: Fashion product images dataset.</li><li><a href=https://cocodataset.org/>COCO</a>: Common Objects in Context dataset for object detection, segmentation, and captioning.</li><li><a href=http://host.robots.ox.ac.uk/pascal/VOC/voc2012/>VOC</a>: Visual Object Classes dataset for object detection, segmentation, and classification.</li><li><a href=https://github.com/fyu/lsun>LSUN</a>: Large-scale Scene Understanding dataset for scene classification and generation.</li><li><a href=http://ufldl.stanford.edu/housenumbers/>SVHN</a>: Street View House Numbers dataset.</li><li><a href=https://cs.stanford.edu/~acoates/stl10/>STL10</a>: Small images dataset with 10 classes.</li><li><a href=https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>CelebA</a>: Large-scale celebrity faces dataset.</li></ul><p>To use these built-in datasets in PyTorch, you need to follow these steps:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>from</span> torchvision <span style=color:#719e07>import</span> datasets
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>dataset <span style=color:#719e07>=</span> datasets<span style=color:#719e07>.</span>MNIST(root<span style=color:#719e07>=</span><span style=color:#2aa198>&#39;./data&#39;</span>, train<span style=color:#719e07>=</span><span style=color:#cb4b16>True</span>, download<span style=color:#719e07>=</span><span style=color:#cb4b16>True</span>)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span><span style=color:#586e75># Access the samples</span>
</span></span><span style=display:flex><span>sample <span style=color:#719e07>=</span> dataset[<span style=color:#2aa198>1</span>]
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#719e07>import</span> matplotlib.pyplot <span style=color:#719e07>as</span> plt  <span style=color:#586e75># Visualize</span>
</span></span><span style=display:flex><span>plt<span style=color:#719e07>.</span>figure(figsize<span style=color:#719e07>=</span>(<span style=color:#2aa198>3</span>,<span style=color:#2aa198>3</span>))
</span></span><span style=display:flex><span>plt<span style=color:#719e07>.</span>imshow(sample[<span style=color:#2aa198>0</span>])
</span></span><span style=display:flex><span>plt<span style=color:#719e07>.</span>show()
</span></span></code></pre></div><h3 id=transformations>Transformations</h3><p>Transformations in PyTorch are operations applied to data samples in a dataset. They are commonly used to preprocess or augment the data before feeding it into a machine learning model. PyTorch provides the <code>torchvision.transforms</code> module, which offers a variety of predefined transformations for computer vision tasks. Here are some commonly used transformations:</p><ul><li><p><code>ToTensor()</code>: Converts a PIL image or numpy array to a PyTorch tensor. It also scales the pixel values between 0 and 1.</p></li><li><p><code>Normalize(mean, std)</code>: Normalizes a tensor by subtracting the mean and dividing by the standard deviation. The mean and std arguments specify the channel-wise means and standard deviations.</p></li><li><p><code>Resize(size)</code>: Resizes the input PIL image to the specified size. It can take a single integer as an argument to resize the image&rsquo;s shorter side while maintaining its aspect ratio.</p></li><li><p><code>CenterCrop(size)</code>: Crops the center portion of the image to the specified size.</p></li><li><p><code>RandomCrop(size)</code>: Randomly crops the input image to the specified size.</p></li><li><p><code>RandomHorizontalFlip()</code>: Randomly flips the input image horizontally with a probability of 0.5.</p></li><li><p><code>RandomRotation(degrees)</code>: Rotates the input image by a random angle within the specified range.</p></li><li><p><code>RandomResizedCrop(size)</code>: Randomly crops and resizes the input image to the specified size.</p></li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>from</span> torchvision <span style=color:#719e07>import</span> transforms
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># Define the transformations</span>
</span></span><span style=display:flex><span>transform <span style=color:#719e07>=</span> transforms<span style=color:#719e07>.</span>Compose([
</span></span><span style=display:flex><span>    transforms<span style=color:#719e07>.</span>CenterCrop(<span style=color:#2aa198>20</span>),  <span style=color:#586e75># Crops the given image at the center.</span>
</span></span><span style=display:flex><span>    transforms<span style=color:#719e07>.</span>Resize(<span style=color:#2aa198>28</span>),  <span style=color:#586e75># Resize the input image to 28*28</span>
</span></span><span style=display:flex><span>    <span style=color:#586e75># transforms.RandomHorizontalFlip(),  # Randomly flip horizontally</span>
</span></span><span style=display:flex><span>    transforms<span style=color:#719e07>.</span>ToTensor(),  <span style=color:#586e75># Convert PIL image to tensor</span>
</span></span><span style=display:flex><span>    transforms<span style=color:#719e07>.</span>Normalize((<span style=color:#2aa198>0.5</span>,), (<span style=color:#2aa198>0.5</span>,))  <span style=color:#586e75># Normalize tensor</span>
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># Create an instance of the dataset with transformations</span>
</span></span><span style=display:flex><span>dataset <span style=color:#719e07>=</span> datasets<span style=color:#719e07>.</span>MNIST(root<span style=color:#719e07>=</span><span style=color:#2aa198>&#39;./data&#39;</span>, train<span style=color:#719e07>=</span><span style=color:#cb4b16>True</span>, download<span style=color:#719e07>=</span><span style=color:#cb4b16>True</span>, transform<span style=color:#719e07>=</span>transform)
</span></span></code></pre></div><h3 id=what-is-dataloader-and-how-to-use>What is DataLoader and how to use</h3><p>In PyTorch, a <code>DataLoader</code> is an iterable that provides an interface to efficiently load data from a dataset during training or evaluation. It handles batch loading, shuffling, and other useful functionalities to facilitate the training process. The <code>DataLoader</code> takes a dataset as input and returns batches of data samples and their corresponding labels.</p><p>The relationship between a dataset and a <code>DataLoader</code> is that the <code>DataLoader</code> wraps the dataset and provides an interface to access the data in batches. It abstracts away the details of data loading and allows you to focus on training your model.</p><p>To use a <code>DataLoader</code> with the dataset you defined, you can follow these steps:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>import</span> torch
</span></span><span style=display:flex><span><span style=color:#719e07>from</span> torch.utils.data <span style=color:#719e07>import</span> DataLoader
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>dataset <span style=color:#719e07>=</span> CustomDataset(data)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>dataloader <span style=color:#719e07>=</span> DataLoader(dataset, batch_size<span style=color:#719e07>=</span><span style=color:#2aa198>32</span>, shuffle<span style=color:#719e07>=</span><span style=color:#cb4b16>True</span>)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#719e07>for</span> batch_data, batch_labels <span style=color:#719e07>in</span> dataloader:
</span></span><span style=display:flex><span>    <span style=color:#586e75># Use the batch_data and batch_labels for training or evaluation</span>
</span></span><span style=display:flex><span>    <span style=color:#586e75># ...</span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>    <span style=color:#586e75># Clear gradients, perform backward pass, update model parameters, etc.</span>
</span></span><span style=display:flex><span>    <span style=color:#586e75># ...</span>
</span></span></code></pre></div><h2 id=training-and-optimization>Training and Optimization</h2><p>Splitting data into training, validation, and test sets
The common method we use for the dataset split is from sklearn</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>from</span> sklearn.model_selection <span style=color:#719e07>import</span> train_test_split  dataset <span style=color:#719e07>=</span> CustomDataset(data)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Split the data into train, validation, and test sets</span>
</span></span><span style=display:flex><span>train_data, test_data <span style=color:#719e07>=</span> train_test_split(dataset, test_size<span style=color:#719e07>=</span><span style=color:#2aa198>0.2</span>, random_state<span style=color:#719e07>=</span><span style=color:#2aa198>42</span>)
</span></span><span style=display:flex><span>train_data, val_data <span style=color:#719e07>=</span> train_test_split(train_data, test_size<span style=color:#719e07>=</span><span style=color:#2aa198>0.2</span>, random_state<span style=color:#719e07>=</span><span style=color:#2aa198>42</span>)
</span></span></code></pre></div><p>In the example above, the dataset is split into 80% training data and 20% test data. Then, the training data is further split into 80% for training and 20% for validation.</p><p>Once you have split your data into training, validation, and test sets, you can construct separate data loaders for each set.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#586e75># Define batch size</span>
</span></span><span style=display:flex><span>batch_size <span style=color:#719e07>=</span> <span style=color:#2aa198>32</span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#586e75># Create data loaders for training, validation, and test sets</span>
</span></span><span style=display:flex><span>train_loader <span style=color:#719e07>=</span> DataLoader(train_data, batch_size<span style=color:#719e07>=</span>batch_size, shuffle<span style=color:#719e07>=</span><span style=color:#cb4b16>True</span>)
</span></span><span style=display:flex><span>val_loader <span style=color:#719e07>=</span> DataLoader(val_data, batch_size<span style=color:#719e07>=</span>batch_size)
</span></span><span style=display:flex><span>test_loader <span style=color:#719e07>=</span> DataLoader(test_data, batch_size<span style=color:#719e07>=</span>batch_size)
</span></span></code></pre></div><p>In the code above, <code>train_data</code>, <code>val_data</code>, and <code>test_data</code> represent the respective splits of your dataset. The <code>batch_size</code> parameter specifies the number of samples in each batch. You can adjust this value based on your computational resources and model requirements.</p><p>The <code>DataLoader</code> class from PyTorch is used to create the data loaders. The <code>shuffle=True</code> argument is passed to the training loader to randomly shuffle the samples in each epoch, which helps in improving the model&rsquo;s generalization.</p><h3 id=defining-loss-functions-eg-cross-entropy-mean-squared-error>Defining loss functions (e.g., cross-entropy, mean squared error)</h3><p>After defining the data loader and model in a deep learning task, the next steps typically involve defining the loss function and optimizer. Here&rsquo;s a general outline of how to do that:</p><p>The loss function measures the discrepancy between the predicted output of your model and the true labels. The choice of loss function depends on the specific task you are working on. Some common loss functions include:</p><ul><li>Mean Squared Error (MSE): Suitable for regression tasks where the output is continuous.</li><li>Binary Cross-Entropy: Used for binary classification tasks where the output is a probability between 0 and 1.</li><li>Categorical Cross-Entropy: Appropriate for multi-class classification problems where the output is a probability distribution over multiple classes.</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#586e75># loss_fn = nn.MSELoss()</span>
</span></span><span style=display:flex><span><span style=color:#586e75># loss_fn = nn.BCELoss()</span>
</span></span><span style=display:flex><span>loss_fn <span style=color:#719e07>=</span> nn<span style=color:#719e07>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span><span style=color:#586e75># loss_fn = nn.KLDivLoss()</span>
</span></span></code></pre></div><h3 id=choosing-and-configuring-optimizers-eg-sgd-adam>Choosing and configuring optimizers (e.g., SGD, Adam)</h3><p>The optimizer is responsible for updating the model&rsquo;s parameters based on the computed gradients during the backpropagation process. It adjusts the parameters in the direction that minimizes the loss function. One commonly used optimizer is Stochastic Gradient Descent (SGD), but there are many other variants available, such as Adam, RMSprop, and Adagrad. PyTorch provides various optimizers in the <code>torch.optim</code> module. To use an optimizer, you typically need to pass the model parameters and specify the learning rate. Here&rsquo;s an example of defining an optimizer:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>import</span> torch.optim <span style=color:#719e07>as</span> optim
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span><span style=color:#586e75># optimizer = optim.SGD(model.parameters(), lr=0.001)</span>
</span></span><span style=display:flex><span><span style=color:#586e75># Or</span>
</span></span><span style=display:flex><span>optimizer <span style=color:#719e07>=</span> optim<span style=color:#719e07>.</span>Adam(model<span style=color:#719e07>.</span>parameters(), lr<span style=color:#719e07>=</span><span style=color:#2aa198>0.001</span>)
</span></span></code></pre></div><h3 id=do-training>Do Training</h3><p>Once you have defined the model, data loader, loss function, and optimizer, you can proceed with training your model and calculating metrics for each batch and epoch. Here&rsquo;s an overview of the steps involved:</p><p>Training Loop: Iterate over your data for a specified number of epochs. In each epoch, iterate over the batches of data provided by the data loader. Here&rsquo;s an example of a training loop:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>device <span style=color:#719e07>=</span> <span style=color:#2aa198>&#39;cuda&#39;</span> <span style=color:#719e07>if</span> torch<span style=color:#719e07>.</span>cuda<span style=color:#719e07>.</span>is_available() <span style=color:#719e07>else</span> <span style=color:#2aa198>&#39;cpu&#39;</span>
</span></span><span style=display:flex><span>model <span style=color:#719e07>=</span> model<span style=color:#719e07>.</span>to(device)
</span></span><span style=display:flex><span>loss_fn <span style=color:#719e07>=</span> loss_fn<span style=color:#719e07>.</span>to(device)
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#719e07>import</span> torch.nn.functional <span style=color:#719e07>as</span> F
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#719e07>def</span> <span style=color:#268bd2>metric</span>(batch_predictions, batch_labels):
</span></span><span style=display:flex><span>    <span style=color:#586e75># Convert the predictions and labels to numpy arrays</span>
</span></span><span style=display:flex><span>    _, predicted_labels <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>max(batch_predictions, dim<span style=color:#719e07>=</span><span style=color:#2aa198>1</span>)
</span></span><span style=display:flex><span>    correct_predictions <span style=color:#719e07>=</span> (predicted_labels <span style=color:#719e07>==</span> batch_labels)<span style=color:#719e07>.</span>sum()<span style=color:#719e07>.</span>item()
</span></span><span style=display:flex><span>    <span style=color:#586e75># Calculate the prediction error</span>
</span></span><span style=display:flex><span>    accuracy <span style=color:#719e07>=</span> correct_predictions <span style=color:#719e07>/</span> <span style=color:#b58900>len</span>(batch_labels)
</span></span><span style=display:flex><span>    <span style=color:#719e07>return</span> accuracy
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>num_epochs <span style=color:#719e07>=</span> <span style=color:#2aa198>10</span>
</span></span><span style=display:flex><span><span style=color:#719e07>for</span> epoch <span style=color:#719e07>in</span> <span style=color:#b58900>range</span>(num_epochs):
</span></span><span style=display:flex><span>    epoch_loss <span style=color:#719e07>=</span> <span style=color:#2aa198>0.0</span>
</span></span><span style=display:flex><span>    epoch_metric <span style=color:#719e07>=</span> <span style=color:#2aa198>0.0</span>
</span></span><span style=display:flex><span>    batch_count <span style=color:#719e07>=</span> <span style=color:#2aa198>0</span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>    <span style=color:#586e75># Iterate over the batches in the dataloader</span>
</span></span><span style=display:flex><span>    <span style=color:#719e07>for</span> batch <span style=color:#719e07>in</span> train_loader:
</span></span><span style=display:flex><span>        <span style=color:#586e75># Clear gradients from previous iteration</span>
</span></span><span style=display:flex><span>        batch_data, batch_labels <span style=color:#719e07>=</span> batch[<span style=color:#2aa198>0</span>], batch[<span style=color:#2aa198>1</span>]
</span></span><span style=display:flex><span>        batch_data <span style=color:#719e07>=</span> batch_data<span style=color:#719e07>.</span>to(device)
</span></span><span style=display:flex><span>        batch_labels <span style=color:#719e07>=</span> batch_labels<span style=color:#719e07>.</span>to(device)
</span></span><span style=display:flex><span>        <span style=color:#586e75># print(batch_data.size())</span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#719e07>.</span>zero_grad()
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>        <span style=color:#586e75># Forward pass</span>
</span></span><span style=display:flex><span>        batch_predictions <span style=color:#719e07>=</span> model(batch_data)
</span></span><span style=display:flex><span>        <span style=color:#586e75># print(batch_predictions)</span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>        <span style=color:#586e75># Calculate loss</span>
</span></span><span style=display:flex><span>        loss <span style=color:#719e07>=</span> loss_fn(batch_predictions, batch_labels)
</span></span><span style=display:flex><span>        epoch_loss <span style=color:#719e07>+=</span> loss<span style=color:#719e07>.</span>item()
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>        <span style=color:#586e75># Backward pass</span>
</span></span><span style=display:flex><span>        loss<span style=color:#719e07>.</span>backward()
</span></span><span style=display:flex><span>        optimizer<span style=color:#719e07>.</span>step()
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>        <span style=color:#586e75># Calculate and record metrics</span>
</span></span><span style=display:flex><span>        batch_metric <span style=color:#719e07>=</span> metric(batch_predictions, batch_labels)
</span></span><span style=display:flex><span>        epoch_metric <span style=color:#719e07>+=</span> batch_metric
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>        batch_count <span style=color:#719e07>+=</span> <span style=color:#2aa198>1</span>
</span></span><span style=display:flex><span>      
</span></span><span style=display:flex><span>    <span style=color:#586e75># Calculate average loss and metric for the epoch</span>
</span></span><span style=display:flex><span>    avg_epoch_loss <span style=color:#719e07>=</span> epoch_loss <span style=color:#719e07>/</span> batch_count
</span></span><span style=display:flex><span>    avg_epoch_metric <span style=color:#719e07>=</span> epoch_metric <span style=color:#719e07>/</span> batch_count
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>    <span style=color:#586e75># Print or log the metrics for analysis</span>
</span></span><span style=display:flex><span>    <span style=color:#b58900>print</span>(<span style=color:#2aa198>f</span><span style=color:#2aa198>&#34;Epoch </span><span style=color:#2aa198>{</span>epoch<span style=color:#719e07>+</span><span style=color:#2aa198>1</span><span style=color:#2aa198>}</span><span style=color:#2aa198> - Loss: </span><span style=color:#2aa198>{</span>avg_epoch_loss<span style=color:#2aa198>:</span><span style=color:#2aa198>.4f</span><span style=color:#2aa198>}</span><span style=color:#2aa198> - Metric: </span><span style=color:#2aa198>{</span>avg_epoch_metric<span style=color:#2aa198>:</span><span style=color:#2aa198>.4f</span><span style=color:#2aa198>}</span><span style=color:#2aa198>&#34;</span>)
</span></span></code></pre></div><p>You&rsquo;ll typically follow these steps:</p><ul><li>Iterate over the training dataset for a specific number of epochs.<ul><li>Within each epoch, iterate over the batches of the dataset.<ul><li>Perform the forward pass through the model to obtain predictions.</li><li>Calculate the loss using the defined loss function.</li><li>Perform the backward pass and update the model parameters using the optimizer.</li></ul></li><li>Calculate and record the desired metrics for analysis.</li></ul></li></ul><h4 id=saving-and-loading-models>Saving and Loading Models</h4><p>To save a trained model in PyTorch, you can use the <code>torch.save()</code> function. This function allows you to save various components of the model, including the model&rsquo;s architecture, parameters, optimizer state, and any additional information you want to store.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#719e07>.</span>save(model, <span style=color:#2aa198>&#39;saved_model.pth&#39;</span>)
</span></span></code></pre></div><p>In this example, the model object is saved in a file called &ldquo;<code>saved_model.pth</code>&rdquo;. This file will contain the entire model, including its architecture, parameters, and other associated information.</p><p>When you load the saved model, you can use the <code>torch.load()</code> function. Here&rsquo;s an example of how to load the saved model:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>loaded_model <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>load(<span style=color:#2aa198>&#39;saved_model.pth&#39;</span>)
</span></span><span style=display:flex><span>loaded_model <span style=color:#719e07>=</span> loaded_model<span style=color:#719e07>.</span>to(device)
</span></span><span style=display:flex><span><span style=color:#586e75># Print the model summary</span>
</span></span><span style=display:flex><span>summary(loaded_model, (<span style=color:#2aa198>1</span>, <span style=color:#2aa198>28</span>, <span style=color:#2aa198>28</span>))  <span style=color:#586e75># Provide an example input size</span>
</span></span></code></pre></div><p>The <code>torch.load()</code> function returns the model object, which you can assign to a variable (loaded_model in this case). After loading, you can use the loaded model for inference, evaluation, or further training.</p><p>Note that when you save the entire model using <code>torch.save()</code>, it saves the complete state of the model, including all parameters and buffers. However, it does not save the optimizer state by default. If you want to save and load the optimizer state as well, you can save it <em>separately</em> or include it in a dictionary along with the model.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#586e75># Save model and optimizer together</span>
</span></span><span style=display:flex><span>checkpoint <span style=color:#719e07>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#2aa198>&#39;model_state_dict&#39;</span>: model<span style=color:#719e07>.</span>state_dict(),
</span></span><span style=display:flex><span>    <span style=color:#2aa198>&#39;optimizer_state_dict&#39;</span>: optimizer<span style=color:#719e07>.</span>state_dict()
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>torch<span style=color:#719e07>.</span>save(checkpoint, <span style=color:#2aa198>&#39;saved_model-w-optimizer.pth&#39;</span>)
</span></span></code></pre></div><p>To load the model and optimizer together, you can use <code>torch.load()</code> and then access the saved states:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>checkpoint <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>load(<span style=color:#2aa198>&#39;saved_model-w-optimizer.pth&#39;</span>)
</span></span><span style=display:flex><span>opt_model <span style=color:#719e07>=</span> MyNetwork(in_channels<span style=color:#719e07>=</span><span style=color:#2aa198>1</span>, out_classes<span style=color:#719e07>=</span><span style=color:#2aa198>10</span>)
</span></span><span style=display:flex><span>opt_optimizer <span style=color:#719e07>=</span> optim<span style=color:#719e07>.</span>Adam(opt_model<span style=color:#719e07>.</span>parameters(), lr<span style=color:#719e07>=</span><span style=color:#2aa198>0.001</span>)
</span></span><span style=display:flex><span>opt_model<span style=color:#719e07>.</span>load_state_dict(checkpoint[<span style=color:#2aa198>&#39;model_state_dict&#39;</span>])
</span></span><span style=display:flex><span>opt_optimizer<span style=color:#719e07>.</span>load_state_dict(checkpoint[<span style=color:#2aa198>&#39;optimizer_state_dict&#39;</span>])
</span></span></code></pre></div><h2 id=transfer-learning>Transfer Learning</h2><p>In PyTorch, you can use predefined models from the <code>torchvision.models</code> module, which provides a collection of popular pre-trained models for tasks such as image classification, object detection, and segmentation (Huggingface for NLP, LLM, and Multi-modal). These models are trained on large datasets like ImageNet and have learned useful features that can be leveraged for various computer vision tasks.</p><h3 id=load-models-and-pre-trained-weights>Load Models and Pre-trained weights</h3><p>To use a predefined model and download its pre-trained parameters, you can follow these steps:</p><p>Import the necessary modules, and load a model:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>import</span> torchvision.models <span style=color:#719e07>as</span> models
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>model <span style=color:#719e07>=</span> models<span style=color:#719e07>.</span>resnet50(weights<span style=color:#719e07>=</span>models<span style=color:#719e07>.</span>ResNet50_Weights<span style=color:#719e07>.</span>IMAGENET1K_V1)
</span></span></code></pre></div><p>In this example, the ResNet-50 model is loaded with pre-trained weights. You can choose different models such as ResNet-18, VGG-16, etc. Here is the full model list: <a href=https://pytorch.org/vision/stable/models.html>Models and pre-trained weights — Torchvision 0.15 documentation (pytorch.org)</a></p><h3 id=modify-model-for-your-task>Modify model for your task</h3><p>After loaded such model with pre-trained parameters, you can use them just like the model you defined, but you have to know that the model is design for a specific tasks, which may not align with yours. So if you want to use the model on your task and fully utilize the pretrain effort, you can change the model and do transfer learning (fine-tuning). What you need to do is replacing or fine-tuning the last fully connected layer. For ResNet model the last layer typically corresponds to the classification layer for ImageNet&rsquo;s 1000 classes. If your task has a different number of classes, you need to adapt the last layer accordingly. For example:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>num_classes <span style=color:#719e07>=</span> <span style=color:#2aa198>10</span>
</span></span><span style=display:flex><span>model<span style=color:#719e07>.</span>fc <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>nn<span style=color:#719e07>.</span>Linear(model<span style=color:#719e07>.</span>fc<span style=color:#719e07>.</span>in_features, num_classes)
</span></span></code></pre></div><p>In this example, the last fully connected layer (<code>model.fc</code>) is replaced with a new linear layer that has num_classes output units.</p><p>Depending on your task and the amount of available data, you may choose to freeze some layers to prevent their weights from being updated during training. This is particularly useful when you have limited data or when the pre-trained model is already well-suited to your task. For example, to freeze all layers except the last one:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>for</span> param <span style=color:#719e07>in</span> model<span style=color:#719e07>.</span>parameters():
</span></span><span style=display:flex><span>    param<span style=color:#719e07>.</span>requires_grad <span style=color:#719e07>=</span> <span style=color:#cb4b16>False</span>
</span></span><span style=display:flex><span>model<span style=color:#719e07>.</span>fc<span style=color:#719e07>.</span>weight<span style=color:#719e07>.</span>requires_grad <span style=color:#719e07>=</span> <span style=color:#cb4b16>True</span>
</span></span><span style=display:flex><span>model<span style=color:#719e07>.</span>fc<span style=color:#719e07>.</span>bias<span style=color:#719e07>.</span>requires_grad <span style=color:#719e07>=</span> <span style=color:#cb4b16>True</span>
</span></span></code></pre></div><p>In this example, all parameters (<code>requires_grad</code>) are set to False except for the weights and biases of the last fully connected layer (<code>model.fc</code>).</p><p>With these steps, you can use a pre-trained model, download its pre-trained parameters, and modify it to suit your specific task. Once you&rsquo;ve made the necessary modifications, you can train the model on your own dataset or use it for inference.</p><blockquote><p>PS: The layer naming is not fully aligned for all model, like the Transformer model have a different name and structure for the output layer, you need to look in to the model first before the modification.</p></blockquote><h2 id=gpu-acceleration>GPU Acceleration</h2><p>GPU acceleration in PyTorch refers to leveraging the computational power of Graphics Processing Units (GPUs) to speed up training and inference processes. GPUs are highly parallel processors capable of performing multiple calculations simultaneously, making them well-suited for tasks involving large-scale matrix computations, such as deep learning.</p><p>To utilize CUDA and GPU-enabled devices in PyTorch, you need to ensure that you have the appropriate hardware (an NVIDIA GPU) and the necessary software dependencies installed, including <code>CUDA Toolkit</code> and <code>cuDNN</code>. Once you have these prerequisites set up, you can follow these steps to move tensors and models to the GPU for faster computation:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>device <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>device(<span style=color:#2aa198>&#34;cuda&#34;</span> <span style=color:#719e07>if</span> torch<span style=color:#719e07>.</span>cuda<span style=color:#719e07>.</span>is_available() <span style=color:#719e07>else</span> <span style=color:#2aa198>&#34;cpu&#34;</span>)
</span></span></code></pre></div><p>This code snippet checks if CUDA is available and assigns the device accordingly. If CUDA is available, the device will be set to <code>"cuda"</code>; otherwise, it will fall back to the CPU.</p><ul><li>Move tensors to the GPU:</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>tensor <span style=color:#719e07>=</span> tensor<span style=color:#719e07>.</span>to(device)
</span></span></code></pre></div><p>This line of code moves a PyTorch tensor to the GPU by calling the <code>to()</code> method and passing the device as an argument. After this operation, computations involving this tensor will be performed on the GPU.</p><ul><li>Move models to the GPU:</li></ul><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#719e07>=</span> model<span style=color:#719e07>.</span>to(device)
</span></span></code></pre></div><p>Similarly, you can move an entire PyTorch model to the GPU using the <code>to()</code> method. This ensures that all model parameters and computations are performed on the GPU.</p><ul><li>Ensure inputs are also on the GPU:</li></ul><p>If you&rsquo;re passing inputs to the model during training or inference, make sure to move those tensors to the GPU as well. For example:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>input_data <span style=color:#719e07>=</span> input_data<span style=color:#719e07>.</span>to(device)
</span></span></code></pre></div><p>By moving tensors and models to the GPU, PyTorch automatically utilizes the GPU&rsquo;s computational power, leading to faster computations compared to running on the CPU. It&rsquo;s important to note that not all operations are automatically GPU-accelerated. PyTorch provides GPU-accelerated implementations for most common operations, but some custom operations may require manual implementation for GPU compatibility.</p><p>When using a GPU, it&rsquo;s also essential to manage GPU memory appropriately, especially when working with large models or batches of data. You may need to optimize your code to minimize memory usage, such as using gradient accumulation or batch splitting techniques.</p><p>Remember to perform necessary operations on the same device (CPU or GPU) to avoid unnecessary data transfers between devices, as it can negatively impact performance.</p><h2 id=addition-topics>Addition Topics</h2><h3 id=random-seed>Random Seed</h3><p>The random seed is a crucial parameter when working with random number generation in PyTorch or any other deep learning framework. It is used to initialize the pseudorandom number generator (PRNG) algorithm, which is responsible for generating random numbers during model training.</p><p>By setting a specific random seed, you can ensure reproducibility of your experiments. When the same random seed is used, the sequence of random numbers generated by the PRNG will be the same across multiple runs. This is important because deep learning models often involve random initialization of weights, dropout, data shuffling, and other stochastic operations. Reproducibility allows you to compare different model configurations, debug code, and share results with others.</p><p>To preserve determinism when working with random seeds in PyTorch, there are a few key points to keep in mind:</p><ul><li>Setting the seed: Before initializing your model or performing any random operations, set the random seed using the <code>torch.manual_seed(seed)</code> function, where <code>seed</code> is an integer value. You can also set the random seed for numpy and other libraries if they are used in conjunction with PyTorch.</li><li>GPU considerations: If you are using GPUs for training, be aware that additional steps may be necessary to ensure deterministic results. For example, you can use <code>torch.backends.cudnn.deterministic = True</code> and <code>torch.backends.cudnn.benchmark = False</code> to disable certain GPU optimizations that introduce non-determinism.</li><li>NumPy interactions: If your code involves interactions between PyTorch and NumPy, be mindful that both libraries have their own random number generators. To maintain determinism, set the random seed for both libraries using <code>np.random.seed(seed)</code> and <code>torch.manual_seed(seed)</code>.</li><li>Non-deterministic operations: While setting a random seed helps control the sources of randomness, some operations in PyTorch may still be non-deterministic, even with a fixed seed. Examples include certain GPU operations or multi-threaded code. In such cases, achieving full determinism may not be possible.</li><li>Library versions: Ensure that you are using the same version of PyTorch and related libraries across different runs. Changes in library versions or underlying algorithms could affect the reproducibility of results, even with the same random seed.</li><li>By being mindful of these considerations and setting the random seed appropriately, you can increase the reproducibility of your PyTorch model training experiments. Your can check more about reproducibility from Torch Document: Reproducibility — PyTorch 2.0 documentation</li></ul><h3 id=dataloader-worker>DataLoader Worker</h3><p>When using a data loader in frameworks like PyTorch or TensorFlow, you often have the option to specify the number of worker processes to use for data loading. These worker processes, also known as data loading threads, are responsible for asynchronously loading and preprocessing data in parallel to speed up the overall data loading process.</p><p>The correct number of worker processes to use depends on various factors, including the characteristics of your dataset, the available computational resources, and the specific requirements of your training or inference pipeline. In general, increasing the number of worker processes can help speed up data loading, especially when the data loading and preprocessing operations are computationally expensive. However, there are practical limitations to consider, such as the number of CPU cores available and the memory requirements of each worker.</p></article></main><div class="pb-10 w-content"><div class="bg-white dark:bg-gray-900 drop-shadow-lg overflow-hidden sm:rounded-lg"><div class="px-4 py-5 sm:px-6"><h3 class="text-lg leading-6 font-medium text-gray-900 dark:text-gray-200"><a id=article-info-card></a>Article Card</h3><p class="mt-1 text-sm text-gray-500">For "<span class=italic>PyTorch Tutorial</span>"</p></div><div class="border-t border-gray-200 dark:border-gray-700"><dl><table class="border-collapse table-fixed w-full"><tbody class="bg-white dark:bg-gray-800"><tr class="px-4 py-5"><th class="text-left border-b border-gray-100 dark:border-gray-700 pl-6 dark:text-gray-500 w-1/3 px-4 py-5 text-sm font-medium text-gray-500">Author</td><td class="border-b font-bold border-gray-100 dark:border-gray-700 p-4 pl-8 text-gray-500 dark:text-gray-400 mt-1 px-4 py-5 text-sm">Zhenghao Wu</td></tr><tr class="px-4 py-5"><th class="text-left border-b border-gray-100 dark:border-gray-700 pl-6 dark:text-gray-500 w-1/3 px-4 py-5 text-sm font-medium text-gray-500">Publish & Update Date</td><td class="font-bold border-b border-gray-100 dark:border-gray-700 p-4 pl-8 text-gray-500 dark:text-gray-400 mt-1 px-4 py-5 text-sm">2024-01-05</td></tr><tr class="px-4 py-5"><th class="text-left border-b border-gray-100 dark:border-gray-700 pl-6 dark:text-gray-500 w-1/3 px-4 py-5 text-sm font-medium text-gray-500">Tags</td><td class="font-bold border-b border-gray-100 dark:border-gray-700 p-4 pl-8 text-gray-500 dark:text-gray-400 mt-1 px-4 py-5 text-sm"><a class="rounded inline-block mt-1 mb-1 py-1 px-2 text-xs font-medium text-white bg-gray-900 visited:text-white dark:visited:text-white" href=https://ecwuuuuu.com/tags/pytorch>PyTorch</a>
<a class="rounded inline-block mt-1 mb-1 py-1 px-2 text-xs font-medium text-white bg-gray-900 visited:text-white dark:visited:text-white" href=https://ecwuuuuu.com/tags/machine-learning>Machine Learning</a>
<a class="rounded inline-block mt-1 mb-1 py-1 px-2 text-xs font-medium text-white bg-gray-900 visited:text-white dark:visited:text-white" href=https://ecwuuuuu.com/tags/tensorflow>Tensorflow</a>
<a class="rounded inline-block mt-1 mb-1 py-1 px-2 text-xs font-medium text-white bg-gray-900 visited:text-white dark:visited:text-white" href=https://ecwuuuuu.com/tags/model>Model</a>
<a class="rounded inline-block mt-1 mb-1 py-1 px-2 text-xs font-medium text-white bg-gray-900 visited:text-white dark:visited:text-white" href=https://ecwuuuuu.com/tags/dataloader>DataLoader</a>
<a class="rounded inline-block mt-1 mb-1 py-1 px-2 text-xs font-medium text-white bg-gray-900 visited:text-white dark:visited:text-white" href=https://ecwuuuuu.com/tags/dataset>Dataset</a>
<a class="rounded inline-block mt-1 mb-1 py-1 px-2 text-xs font-medium text-white bg-gray-900 visited:text-white dark:visited:text-white" href=https://ecwuuuuu.com/tags/mlp>MLP</a>
<a class="rounded inline-block mt-1 mb-1 py-1 px-2 text-xs font-medium text-white bg-gray-900 visited:text-white dark:visited:text-white" href=https://ecwuuuuu.com/tags/transfer-learning>Transfer Learning</a>
<a class="rounded inline-block mt-1 mb-1 py-1 px-2 text-xs font-medium text-white bg-gray-900 visited:text-white dark:visited:text-white" href=https://ecwuuuuu.com/tags/tutorial>Tutorial</a>
<a class="rounded inline-block mt-1 mb-1 py-1 px-2 text-xs font-medium text-white bg-gray-900 visited:text-white dark:visited:text-white" href=https://ecwuuuuu.com/tags/deep-learning>Deep Learning</a></td></tr><tr class="px-4 py-5"><th class="text-left border-b border-gray-100 dark:border-gray-700 pl-6 dark:text-gray-500 w-1/3 px-4 py-5 text-sm font-medium text-gray-500">License</td><td class="font-bold border-b border-gray-100 dark:border-gray-700 p-4 pl-8 text-gray-500 dark:text-gray-400 mt-1 px-4 py-5 text-sm"><ul role=list class="border border-gray-200 dark:border-gray-700 rounded-md divide-y divide-gray-200 dark:divide-gray-700"><li class="pl-3 pr-4 py-3 flex flex-col sm:flex-row items-center justify-between text-sm"><p class=w-3/4>除非另有说明，本页面上的内容已采用知识共享署名 - 相同方式共享 4.0 国际许可协议进行许可。
Except where otherwise noted, content on this page is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</p><svg height="31" width="88" viewBox="0 0 88 31"><link xmlns="" type="text/css" id="dark-mode" rel="stylesheet" href=""/><title>Creative Commons “Attribution-Share Alike” license icon</title><path d="m2.499.352L85.626.5c1.161.0 2.198-.173 2.198 2.333l-.102 27.552h-87.321V2.73c0-1.235.119-2.378 2.098-2.378z" fill="#AAB2AB"/><path d="m25.316 14.449c.003 5.557-4.471 10.065-9.993 10.069-5.522.003-10.001-4.5-10.005-10.057v-.012C5.315 8.891 9.789 4.383 15.312 4.38c5.522-.004 10.001 4.5 10.005 10.057-.001.003-.001.007-.001.012zM46.464 3.306c4.349.0 7.875 3.548 7.875 7.925s-3.526 7.926-7.875 7.926c-4.35.0-7.875-3.548-7.875-7.926-.001-4.377 3.525-7.925 7.875-7.925zm28.632 7.751c.003 4.314-3.47 7.814-7.757 7.818-4.286.003-7.765-3.492-7.769-7.806v-.012c-.002-4.314 3.471-7.814 7.758-7.817s7.765 3.492 7.768 7.806v.011z" fill="#FFF"/><path d="m23.446 6.252c2.217 2.232 3.326 4.964 3.326 8.197s-1.089 5.936-3.269 8.11c-2.313 2.289-5.046 3.434-8.2 3.434-3.116.0-5.802-1.135-8.057-3.405-2.256-2.271-3.383-4.982-3.383-8.138S4.99 8.561 7.246 6.252c2.198-2.232 4.884-3.348 8.057-3.348 3.212.0 5.926 1.116 8.143 3.348zM8.739 7.753c-1.875 1.905-2.812 4.138-2.812 6.698.0 2.561.928 4.773 2.783 6.64s4.064 2.801 6.627 2.801 4.791-.942 6.684-2.829c1.797-1.752 2.697-3.955 2.697-6.611.0-2.636-.914-4.874-2.74-6.712s-4.04-2.757-6.641-2.757-4.801.923-6.598 2.77zm4.933 5.572c-.287-.628-.715-.942-1.287-.942-1.011.0-1.516.685-1.516 2.054.0 1.37.505 2.055 1.516 2.055.667.0 1.145-.333 1.431-1.002l1.401.751c-.668 1.194-1.67 1.792-3.006 1.792-1.03.0-1.856-.317-2.476-.954-.621-.636-.931-1.512-.931-2.629.0-1.099.32-1.97.959-2.616s1.436-.968 2.39-.968c1.413.0 2.424.56 3.035 1.679l-1.516.78zm6.593.0c-.287-.628-.707-.942-1.261-.942-1.031.0-1.547.685-1.547 2.054.0 1.37.516 2.055 1.547 2.055.669.0 1.137-.333 1.404-1.002l1.433.751c-.667 1.194-1.667 1.792-3.001 1.792-1.029.0-1.853-.317-2.473-.954-.619-.636-.928-1.512-.928-2.629.0-1.099.314-1.97.943-2.616.628-.646 1.428-.968 2.4-.968 1.41.0 2.42.56 3.029 1.679l-1.546.78zM86.353.0h-84.706c-.908.0-1.647.744-1.647 1.658v28.967c0 .207.167.375.372.375h87.256c.205.0.372-.168.372-.375V1.658c0-.914-.739-1.658-1.647-1.658zm-84.706.749h84.705c.498.0.903.408.903.909v20.109H26.714c-2.219 4.038-6.494 6.779-11.401 6.779-4.908.0-9.183-2.738-11.4-6.779h-3.169V1.658c0-.501.405-.909.903-.909zM67.277 2.5c-2.355.0-4.349.827-5.98 2.481-1.675 1.712-2.512 3.737-2.512 6.077s.837 4.351 2.512 6.034c1.674 1.683 3.668 2.524 5.98 2.524 2.342.0 4.371-.849 6.089-2.546 1.616-1.611 2.427-3.616 2.427-6.012s-.824-4.422-2.471-6.077C71.677 3.327 69.662 2.5 67.277 2.5zm.022 1.54c1.93.0 3.569.685 4.918 2.054 1.361 1.355 2.043 3.01 2.043 4.964.0 1.968-.666 3.602-2.001 4.9-1.405 1.397-3.058 2.096-4.96 2.096-1.901.0-3.541-.691-4.917-2.074-1.376-1.384-2.064-3.024-2.064-4.921s.695-3.552 2.086-4.964c1.332-1.371 2.965-2.055 4.895-2.055zm-3.791 5.809c.34-2.153 1.846-3.304 3.733-3.304 2.716.0 4.369 1.982 4.369 4.626.0 2.58-1.76 4.584-4.411 4.584-1.824.0-3.457-1.13-3.755-3.347h2.143c.063 1.151.806 1.556 1.866 1.556 1.209.0 1.994-1.13 1.994-2.857.0-1.812-.679-2.771-1.951-2.771-.934.0-1.739.341-1.909 1.513l.623-.003-1.687 1.697-1.686-1.697.671.003zm-14.765-.911c0-.306-.246-.553-.55-.553h-3.478c-.303.0-.55.247-.55.553v3.5h.971v4.145h2.636v-4.145h.971v-3.5zM46.455 5.53c.656.0 1.189.536 1.189 1.197s-.533 1.197-1.189 1.197c-.657.0-1.189-.536-1.189-1.197s.532-1.197 1.189-1.197zm-.012-3.03c-2.355.0-4.349.827-5.981 2.481-1.675 1.711-2.512 3.737-2.512 6.076s.837 4.351 2.512 6.034c1.674 1.683 3.668 2.524 5.981 2.524 2.342.0 4.371-.849 6.088-2.547 1.619-1.611 2.428-3.615 2.428-6.012s-.823-4.421-2.47-6.076c-1.645-1.654-3.661-2.48-6.046-2.48zm.022 1.539c1.93.0 3.569.685 4.917 2.054 1.363 1.355 2.044 3.01 2.044 4.963.0 1.968-.666 3.602-2.001 4.9-1.405 1.398-3.058 2.096-4.96 2.096-1.901.0-3.541-.691-4.917-2.075-1.377-1.383-2.065-3.023-2.065-4.921.0-1.896.695-3.551 2.086-4.963 1.334-1.369 2.966-2.054 4.896-2.054z"/><path d="m69.277 24.171 1.816 4.888h-1.109l-.367-1.089h-1.816l-.381 1.089h-1.074l1.836-4.888h1.095zm.062 2.997-.612-1.793h-.014l-.633 1.793h1.259zm-6.079.682c.059.115.137.207.234.277.098.071.211.124.342.158.133.034.268.051.408.051.095.0.197-.008.306-.023s.21-.047.306-.093c.094-.046.174-.108.236-.188.064-.08.096-.181.096-.305.0-.132-.042-.238-.126-.321-.083-.083-.194-.15-.329-.206-.136-.055-.29-.102-.461-.143-.173-.042-.348-.088-.523-.138-.182-.046-.358-.102-.531-.167-.171-.066-.325-.152-.461-.258-.137-.104-.246-.235-.33-.393-.083-.158-.125-.349-.125-.572.0-.252.053-.469.16-.654.105-.184.246-.338.418-.462.172-.123.366-.214.584-.274.217-.059.436-.088.652-.088.254.0.497.028.73.086.232.057.44.149.621.277.182.127.326.291.432.49.107.198.16.439.16.723h-1.036c-.009-.146-.04-.268-.091-.363-.053-.096-.121-.172-.207-.227s-.184-.094-.295-.115c-.11-.023-.23-.035-.361-.035-.086.0-.172.01-.258.027-.086.019-.163.051-.232.096-.07.047-.129.104-.174.172s-.067.155-.067.26c0 .096.019.174.054.232.037.061.109.115.215.165s.254.101.441.151c.188.049.434.113.736.191.092.018.217.051.377.1.161.047.32.123.479.229.159.105.296.246.412.422.115.176.173.4.173.674.0.225-.044.432-.13.623-.086.192-.214.357-.384.496-.171.141-.381.248-.632.326-.252.078-.544.116-.874.116-.268.0-.527-.033-.779-.1-.251-.065-.474-.171-.667-.312-.192-.143-.346-.323-.459-.543-.113-.219-.168-.479-.163-.78h1.036c-.001.165.028.304.087.418zm-17.287-3.679h1.198l1.138 1.931 1.13-1.931h1.19l-1.803 3.012v1.876h-1.07v-1.903l-1.783-2.985zm-1.975.0c.231.0.442.021.633.062s.354.108.491.201c.136.094.241.219.316.373.075.155.112.348.112.575.0.247-.055.451-.167.616-.11.164-.276.298-.493.402.3.088.523.239.672.456.148.218.223.479.223.784.0.246-.049.46-.144.641-.095.18-.224.327-.386.441-.161.114-.346.199-.552.254-.207.055-.419.082-.638.082h-2.358V24.17h2.291v.001zm-.137 1.976c.191.0.347-.046.47-.136.123-.092.185-.239.185-.444.0-.114-.021-.208-.062-.28s-.095-.129-.164-.17-.146-.07-.235-.086-.181-.023-.276-.023h-1v1.14h1.082zm.062 2.075c.105.0.205-.01.3-.03.095-.021.18-.055.252-.104.073-.047.13-.112.174-.194s.065-.187.065-.315c0-.25-.071-.43-.212-.536-.141-.107-.328-.161-.559-.161h-1.166v1.341h1.146z" fill="#FFF"/></svg></li></ul></td></tr></tbody></table></div></div></div><div class="pb-10 w-content"><script src=https://giscus.app/client.js data-repo=ecwu/ecwu.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkxNjQzMzE1NjY=" data-category="Blog Post Discussion" data-category-id=DIC_kwDOCcuALs4CObNl data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-input-position=botton data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></div><section class=mb-10><hr><h2 class="text-gray-900 dark:text-gray-200">Related Posts</h2><ul class="list-disc list-inside text-gray-900 dark:text-gray-200"><li><a href=https://ecwuuuuu.com/post/gpu-container-setup-note/>服务器新装 GPU 的容器化配置笔记</a></li><li><a href=https://ecwuuuuu.com/post/obs-studio-show-keyboard-and-mouse-activity/>使用 input-overlay 工具在 OBS Studio 中展示鼠标和按键操作</a></li><li><a href=https://ecwuuuuu.com/post/sigmoid-softmax-binary-class/>Sigmoid or Softmax for Binary Classification</a></li></ul></section><div><footer class="relative mb-6 text-gray-900 dark:text-gray-200"><hr class=pb-5><div class="flex justify-begin w-full mx-auto mb-2"><ul class="flex flex-row justify-begin text-lg space-x-2"><li><a href=https://ecwuuuuu.com/index.xml aria-label=RSS class="round-button social-button"><svg width="16" height="16" fill="currentColor" class="w-5 h-5 fill-gray-700 hover:fill-gray-900 dark:fill-gray-400 dark:hover:fill-gray-200" viewBox="0 0 16 16"><path d="M2 0A2 2 0 000 2v12a2 2 0 002 2h12a2 2 0 002-2V2a2 2 0 00-2-2H2zm1.5 2.5c5.523.0 10 4.477 10 10a1 1 0 11-2 0 8 8 0 00-8-8 1 1 0 010-2zm0 4a6 6 0 016 6 1 1 0 11-2 0 4 4 0 00-4-4 1 1 0 010-2zm.5 7a1.5 1.5.0 110-3 1.5 1.5.0 010 3z"/></svg></a></li></ul></div><p>Powered by <a href=https://gohugo.io>Hugo</a> and <a href=https://github.com/ecwu/ecwu-theme>ecwu-theme</a>.
Build by GitHub Actions at: 2025-10-28 19:21 CST.</p><p>&copy;2025
ECWUUUUU.
All rights reserved.</p><p><a href=http://beian.miit.gov.cn/>粤 ICP 备 17015575 号 - 1</a></p><p class="text-sm my-4">Google Analytics enabled | <a href=https://ecwuuuuu.com/page/changelog>site changelog</a></p></footer></div></body></html>